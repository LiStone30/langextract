#!/usr/bin/env python3
# Copyright 2025 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Quick-start example for using Ollama with langextract."""

import argparse
import os
import sys
import json

import langextract as lx


def run_extraction(model_id="qwen3:8b", temperature=0.3):
  """Run a simple extraction example using Ollama."""
  # input_text = "Isaac Asimov was a prolific science fiction writer."
  input_text = "æ™‹æ§åˆ›åŠ›ï¼ˆå±±è¥¿æ™‹æ§è£…å¤‡åˆ›åŠ›æ™ºèƒ½åˆ¶é€ æœ‰é™å…¬å¸ä»‹ç»ï¼‰æˆç«‹äº2021å¹´9æœˆ30æ—¥ï¼Œæ³¨å†Œåœ°ä½äºé•¿æ²»å¸‚ç»æµæŠ€æœ¯å¼€å‘åŒºï¼Œæ³¨å†Œèµ„æœ¬10000ä¸‡å…ƒã€‚å…¬å¸åœ¨ç…¤çŸ¿æœºæ¢°é¢†åŸŸç§¯ææ¢ç´¢æ™ºèƒ½åŒ–ã€ç»¿è‰²åŒ–è½¬å‹ï¼Œè‡´åŠ›äºæ¨åŠ¨ç…¤æœºè£…å¤‡å‡çº§å’Œåˆ¶é€ æ¨¡å¼åˆ›æ–°ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡æŠ€æœ¯åˆ›æ–°å’Œå·¥è‰ºä¼˜åŒ–ï¼Œæå‡ç…¤æœºäº§å“çš„æ™ºèƒ½åŒ–æ°´å¹³ï¼ŒåŒæ—¶é™ä½èƒ½è€—ä¸æ’æ”¾ï¼ŒåŠ©åŠ›è¡Œä¸šå¯æŒç»­å‘å±•ã€‚è¿™ä¸€æ–¹å‘æ—¢ç¬¦åˆå›½å®¶æ¨åŠ¨åˆ¶é€ ä¸šæ™ºèƒ½åŒ–ã€ç»¿è‰²åŒ–å‡çº§çš„æ”¿ç­–è¦æ±‚ï¼Œä¹Ÿæ˜¯æˆ‘ä»¬ç«‹è¶³è¡Œä¸šå®é™…ã€ç¨³æ­¥æ¨è¿›äº§ä¸šç°ä»£åŒ–çš„é‡è¦è·¯å¾„ã€‚å…¬å¸ä»¥æ‰“é€ é«˜ç«¯æ™ºèƒ½å¼€é‡‡æ§åˆ¶æŠ€æœ¯è£…å¤‡äº§å“ä¸ºä¸»ï¼Œç ”å‘ã€åˆ¶é€ ç»¼é‡‡å·¥ä½œé¢æ¶²å‹æ”¯æ¶ç”µæ¶²æ§ç³»ç»Ÿã€æ™ºèƒ½åŒ–æ§åˆ¶ç³»ç»Ÿã€é›†ä¸­ä¾›æ¶²ç³»ç»Ÿã€é«˜ç«¯æ™ºèƒ½ä¹³åŒ–æ¶²æ³µç«™ï¼Œé«˜ç«¯æ™ºèƒ½å–·é›¾æ³µç«™ã€‚"
  

  # prompt = "Extract the author's full name and their primary literary genre."
  prompt = "æå–å‡ºå…¬å¸ã€äº§å“çš„åç§°ã€ç”¨é€”ã€æŠ€æœ¯å‚æ•°ã€ä½¿ç”¨æ–¹æ³•ã€ç‰¹ç‚¹ã€ä¼˜åŠ¿ç­‰å¯¹å…¬å¸å’Œäº§å“è¿›è¡Œä»‹ç»çš„ä¿¡æ¯ã€‚"
  

  examples = [
      lx.data.ExampleData(
          text=(
              "å±±è¥¿äº‘æ™Ÿç§‘æŠ€æœ‰é™å…¬å¸æˆç«‹äº2015å¹´ï¼Œåè½äºæœ‰å¤ªè¡Œæ˜ç ä¹‹ç§°çš„å±±è¥¿çœæ™‹åŸå¸‚ï¼Œå¹¶å…ˆååœ¨æ­¦æ±‰å¸‚ã€è¥¿å®‰å¸‚ã€é•¿æ²»å¸‚ã€ä¸´æ±¾å¸‚ã€é˜³æ³‰å¸‚æˆç«‹å­åˆ†å…¬å¸ã€‚å…¬å¸æ³¨å†Œèµ„é‡‘1000ä¸‡ã€‚        äº‘æ™Ÿç§‘æŠ€æ˜¯å›½å†…è§£å†³å·¥ä¸šè¡Œä¸šå®æ“åŸ¹è®­çš„ç§‘æŠ€å…¬å¸ã€‚å…¬å¸è‡´åŠ›äºå°†AI+XR(VRã€MRã€APPç­‰)é«˜å°–ç«¯æŠ€æœ¯åº”ç”¨äºå®‰å…¨æ•™è‚²ä¸åŸ¹è®­é¢†åŸŸï¼Œå¸®åŠ©ä¼ä¸šå’Œä¸ªäººæ›´é«˜æ•ˆã€å®‰å…¨ã€çœŸå®çš„ä½“éªŒã€å­¦ä¹ ã€‚     "
          ),
          extractions=[
              lx.data.Extraction(
                  extraction_class="å…¬å¸ä»‹ç»",
                  extraction_text=(
                      "å±±è¥¿äº‘æ™Ÿç§‘æŠ€æœ‰é™å…¬å¸æˆç«‹äº2015å¹´ï¼Œåè½äºæœ‰å¤ªè¡Œæ˜ç ä¹‹ç§°çš„å±±è¥¿çœæ™‹åŸå¸‚ï¼Œå¹¶å…ˆååœ¨æ­¦æ±‰å¸‚ã€è¥¿å®‰å¸‚ã€é•¿æ²»å¸‚ã€ä¸´æ±¾å¸‚ã€é˜³æ³‰å¸‚æˆç«‹å­åˆ†å…¬å¸ã€‚å…¬å¸æ³¨å†Œèµ„é‡‘1000ä¸‡ã€‚        äº‘æ™Ÿç§‘æŠ€æ˜¯å›½å†…è§£å†³å·¥ä¸šè¡Œä¸šå®æ“åŸ¹è®­çš„ç§‘æŠ€å…¬å¸ã€‚å…¬å¸è‡´åŠ›äºå°†AI+XR(VRã€MRã€APPç­‰)é«˜å°–ç«¯æŠ€æœ¯åº”ç”¨äºå®‰å…¨æ•™è‚²ä¸åŸ¹è®­é¢†åŸŸï¼Œå¸®åŠ©ä¼ä¸šå’Œä¸ªäººæ›´é«˜æ•ˆã€å®‰å…¨ã€çœŸå®çš„ä½“éªŒã€å­¦ä¹ ã€‚     "
                  ),
                  attributes={
                      "name": "å±±è¥¿äº‘æ™Ÿç§‘æŠ€æœ‰é™å…¬å¸",
                      "genre": "äº‘æ™Ÿç§‘æŠ€æ˜¯å›½å†…è§£å†³å·¥ä¸šè¡Œä¸šå®æ“åŸ¹è®­çš„ç§‘æŠ€å…¬å¸",
                  },
              )
          ],
      )
  ]

  model_config = lx.factory.ModelConfig(
      model_id=model_id,
      provider_kwargs={
          "model_url": os.getenv("OLLAMA_HOST", "http://localhost:11434"),
          "format_type": lx.data.FormatType.JSON,
          "temperature": temperature,
      },
  )

  result = lx.extract(
      text_or_documents=input_text,
      prompt_description=prompt,
      examples=examples,
      config=model_config,
      use_schema_constraints=True,
  )

  # Option 2: Pass model_id directly (simpler)
  # result = lx.extract(
  #     text_or_documents=input_text,
  #     prompt_description=prompt,
  #     examples=examples,
  #     model_id=model_id,
  #     model_url=os.getenv("OLLAMA_HOST", "http://localhost:11434"),
  #     format_type=lx.data.FormatType.JSON,
  #     temperature=temperature,
  #     use_schema_constraints=True,
  # )

  return result


def save_and_visualize_results(result, model_id, temperature):
  """Save results to files and generate interactive visualization."""
  
  # ä¿å­˜ JSON ç»“æœåˆ°æ–‡ä»¶
  output_data = {
    "model_id": model_id,
    "temperature": temperature,
    "input_text": "Isaac Asimov was a prolific science fiction writer.",
    "extractions": []
  }
  
  for extraction in result.extractions:
    output_data["extractions"].append({
      "class": extraction.extraction_class,
      "text": extraction.extraction_text,
      "attributes": extraction.attributes,
      "char_interval": {
        "start": extraction.char_interval.start_pos if extraction.char_interval else None,
        "end": extraction.char_interval.end_pos if extraction.char_interval else None
      } if extraction.char_interval else None
    })
  
  # ä¿å­˜åˆ° JSON æ–‡ä»¶
  output_file = f"extraction_result_{model_id.replace(':', '_')}.json"
  with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(output_data, f, indent=2, ensure_ascii=False)
  
  print(f"ğŸ’¾ JSON ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
  
  # ä¿å­˜ä¸º JSONL æ ¼å¼ç”¨äºå¯è§†åŒ–
  jsonl_file = f"extraction_result_{model_id.replace(':', '_')}.jsonl"
  lx.io.save_annotated_documents([result], output_name=jsonl_file, output_dir=".")
  print(f"ğŸ“Š JSONL æ–‡ä»¶å·²ä¿å­˜åˆ°: {jsonl_file}")
  
  # ç”Ÿæˆäº¤äº’å¼å¯è§†åŒ–
  try:
    print("ğŸ¨ æ­£åœ¨ç”Ÿæˆäº¤äº’å¼å¯è§†åŒ–...")
    html_content = lx.visualize(jsonl_file)
    
    html_file = f"extraction_visualization_{model_id.replace(':', '_')}.html"
    with open(html_file, "w", encoding='utf-8') as f:
      if hasattr(html_content, 'data'):
        f.write(html_content.data)  # For Jupyter/Colab
      else:
        f.write(html_content)
    
    print(f"ğŸŒ äº¤äº’å¼å¯è§†åŒ–å·²ä¿å­˜åˆ°: {html_file}")
    print(f"   è¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ {html_file} æŸ¥çœ‹å¯è§†åŒ–ç»“æœ")
    
  except Exception as e:
    print(f"âš ï¸  å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
    print("   ä½† JSON å’Œ JSONL æ–‡ä»¶å·²æˆåŠŸä¿å­˜")


def main():
  """Main function to run the quick-start example."""
  parser = argparse.ArgumentParser(
      description="Run Ollama extraction example",
      epilog=(
          "Supported models: qwen3:8b, gemma2:2b, llama3.2:1b, mistral:7b, qwen2.5:0.5b,"
          " etc."
      ),
  )
  parser.add_argument(
      "--model-id",
      default=os.getenv("MODEL_ID", "qwen3:8b"),
      help="Ollama model ID (default: qwen3:8b or MODEL_ID env var)",
  )
  parser.add_argument(
      "--temperature",
      type=float,
      default=float(os.getenv("TEMPERATURE", "0.3")),
      help="Model temperature (default: 0.3 or TEMPERATURE env var)",
  )
  parser.add_argument(
      "--visualize",
      action="store_true",
      help="Generate interactive HTML visualization",
  )
  args = parser.parse_args()

  print(f"ğŸš€ Running Ollama quick-start example with {args.model_id}...")
  print("-" * 50)

  try:
    result = run_extraction(
        model_id=args.model_id, temperature=args.temperature
    )

    if result.extractions:
      print(f"\nğŸ“ Found {len(result.extractions)} extraction(s):\n")
      for extraction in result.extractions:
        print(f"Class: {extraction.extraction_class}")
        print(f"Text: {extraction.extraction_text}")
        print(f"Attributes: {extraction.attributes}")
        print()
      
      # ä¿å­˜ç»“æœå¹¶ç”Ÿæˆå¯è§†åŒ–
      if args.visualize:
        save_and_visualize_results(result, args.model_id, args.temperature)
      else:
        # åªä¿å­˜ JSON æ–‡ä»¶
        output_data = {
          "model_id": args.model_id,
          "temperature": args.temperature,
          "input_text": "Isaac Asimov was a prolific science fiction writer.",
          "extractions": []
        }
        
        for extraction in result.extractions:
          output_data["extractions"].append({
            "class": extraction.extraction_class,
            "text": extraction.extraction_text,
            "attributes": extraction.attributes,
            "char_interval": {
              "start": extraction.char_interval.start_pos if extraction.char_interval else None,
              "end": extraction.char_interval.end_pos if extraction.char_interval else None
            } if extraction.char_interval else None
          })
        
        output_file = f"extraction_result_{args.model_id.replace(':', '_')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
          json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ JSON ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        print("ğŸ’¡ ä½¿ç”¨ --visualize å‚æ•°å¯ä»¥ç”Ÿæˆäº¤äº’å¼ HTML å¯è§†åŒ–")
      
    else:
      print("\nâš ï¸  No extractions found")

    print("âœ… SUCCESS! Ollama is working with langextract")
    print(f"   Model: {args.model_id}")
    print("   JSON mode: enabled")
    print("   Schema constraints: enabled")
    return True

  except ConnectionError as e:
    print(f"\nâŒ ConnectionError: {e}")
    print("\nğŸ’¡ Make sure Ollama is running:")
    print("   ollama serve")
    return False
  except ValueError as e:
    if "Can't find Ollama" in str(e):
      print(f"\nâŒ Model not found: {args.model_id}")
      print("\nğŸ’¡ Install the model first:")
      print(f"   ollama pull {args.model_id}")
    else:
      print(f"\nâŒ ValueError: {e}")
    return False
  except Exception as e:
    print(f"\nâŒ Error: {type(e).__name__}: {e}")
    return False


if __name__ == "__main__":
  SUCCESS = main()
  sys.exit(0 if SUCCESS else 1)
