import langextract as lx
import textwrap
import os
from collections import Counter, defaultdict
import docx

"""
ä»æ–‡æœ¬ä¸­æå–å®ä½“ï¼Œä½†æ˜¯ä¸å±€é™äºæŸäº›å®ä½“ï¼Œæ¯”å¦‚å…¬å¸ã€äº§å“ã€å…³ç³»ç­‰ã€‚
å¥å­ä¸­çš„ åè¯ éƒ½å¯ä»¥ç®—ä½œå®ä½“
åŸºäºæ–‡æ¡£ æ¥æ„å»º æœ¬åœ°çŸ¥è¯†å›¾è°± ï¼Œ åŒ…å«äº†å¤æ‚äº†å®ä½“å…³ç³»

æŸ¥è¯¢æ—¶ ä¹Ÿæ˜¯åŸºäºçŸ¥è¯†å›¾è°±è¿›è¡ŒæŸ¥è¯¢

è¦åŸºäºå•ä¸ªå¥å­ è¯†åˆ«å‡º å¤šä¸ªå®ä½“å’Œå…³ç³»
è¦åŸºäºå¤šä¸ªå¥å­ å®ç°ä»£è¯ å’Œ å®ä½“ çš„ å…³ç³»è¯†åˆ«
è¦å°†å±æ€§ å’Œ å…³ç³» åŒºåˆ†å¼€æ¥
åªæœ‰ä¸€ä¸ªå±æ€§ï¼ˆåˆ«åï¼‰
åˆ«çš„éƒ½æ˜¯å…³ç³»


å±±è¥¿äº‘æ™Ÿç§‘æŠ€æœ‰é™å…¬å¸æˆç«‹äº2015å¹´ï¼Œåè½äºæœ‰å¤ªè¡Œæ˜ç ä¹‹ç§°çš„å±±è¥¿çœæ™‹åŸå¸‚ï¼Œå¹¶å…ˆååœ¨æ­¦æ±‰å¸‚ã€è¥¿å®‰å¸‚ã€é•¿æ²»å¸‚ã€ä¸´æ±¾å¸‚ã€é˜³æ³‰å¸‚æˆç«‹å­åˆ†å…¬å¸ã€‚

å…³ç³»
lx.data.ExampleData(
          text=(
              å±±è¥¿äº‘æ™Ÿç§‘æŠ€æœ‰é™å…¬å¸æˆç«‹äº2015å¹´ï¼Œåè½äºæœ‰å¤ªè¡Œæ˜ç ä¹‹ç§°çš„å±±è¥¿çœæ™‹åŸå¸‚ï¼Œå¹¶å…ˆååœ¨æ­¦æ±‰å¸‚ã€è¥¿å®‰å¸‚ã€é•¿æ²»å¸‚ã€ä¸´æ±¾å¸‚ã€é˜³æ³‰å¸‚æˆç«‹å­åˆ†å…¬å¸ã€‚å…¬å¸æ³¨å†Œèµ„é‡‘1000ä¸‡ã€‚"
          ),
          extractions=[
              lx.data.Extraction(
                  extraction_class="relationship",
                  extraction_text=(
                      "å±±è¥¿äº‘æ™Ÿç§‘æŠ€æœ‰é™å…¬å¸æˆç«‹äº2015å¹´"
                  ),
                  attributes={
                      "type": "æˆç«‹æ—¶é—´",
                      "entity_1": "å±±è¥¿äº‘æ™Ÿç§‘æŠ€æœ‰é™å…¬å¸",
                      "entity_2": "2015å¹´",
                  },
              ),
              lx.data.Extraction(
                  extraction_class="relationship",
                  extraction_text=(
                      "åè½äºæœ‰å¤ªè¡Œæ˜ç ä¹‹ç§°çš„å±±è¥¿çœæ™‹åŸå¸‚"
                  ),
                  attributes={
                      "type": "æˆç«‹åœ°ç‚¹",
                      "entity_1": "å±±è¥¿äº‘æ™Ÿç§‘æŠ€æœ‰é™å…¬å¸",
                      "entity_2": "å±±è¥¿çœæ™‹åŸå¸‚",
                  },
              ),
              lx.data.Extraction(
                  extraction_class="relationship",
                  extraction_text=(
                      "å¹¶å…ˆååœ¨æ­¦æ±‰å¸‚ã€è¥¿å®‰å¸‚ã€é•¿æ²»å¸‚ã€ä¸´æ±¾å¸‚ã€é˜³æ³‰å¸‚æˆç«‹å­åˆ†å…¬å¸ã€‚"
                  ),
                  attributes={
                      "type": "åˆ†å…¬å¸åœ°ç‚¹",
                      "entity_1": "å±±è¥¿äº‘æ™Ÿç§‘æŠ€æœ‰é™å…¬å¸",
                      "entity_2": "æ­¦æ±‰å¸‚ã€è¥¿å®‰å¸‚ã€é•¿æ²»å¸‚ã€ä¸´æ±¾å¸‚ã€é˜³æ³‰å¸‚",
                  },
              ),
              lx.data.Extraction(
                  extraction_class="relationship",
                  extraction_text=(
                      "å…¬å¸æ³¨å†Œèµ„é‡‘1000ä¸‡ã€‚"
                  ),
                  attributes={
                      "type": "æ³¨å†Œèµ„é‡‘",
                      "entity_1": "å±±è¥¿äº‘æ™Ÿç§‘æŠ€æœ‰é™å…¬å¸",
                      "entity_2": "1000ä¸‡",
                  },
              ),
           
          ],
      )

"""


# Define comprehensive prompt and examples for complex literary text
prompt = textwrap.dedent("""\
    ä»æ–‡æœ¬ä¸­æå–å®ä½“å…³ç³»ï¼Œå•ä¸ªå¥å­å¥å­å¯ä»¥æå–å‡ºå¤šä¸ªå®ä½“å…³ç³»ã€‚
    å¯¹äºä»£è¯åº”è¯¥æ‰¾åˆ°å¯¹åº”çš„å®ä½“ï¼Œå¹¶è¯†åˆ«å‡ºå…³ç³»
    """)
    # é‡è¦è¦æ±‚ï¼š
    # 1. å¿…é¡»åŒ…å« "extractions" é”®ï¼Œä¸”å¿…é¡»æ˜¯æ•°ç»„
    # 2. æ‰€æœ‰å­—ç¬¦ä¸²å€¼å¿…é¡»ç”¨è‹±æ–‡åŒå¼•å·åŒ…å›´
    # 3. ç¡®ä¿JSONç»“æ„å®Œæ•´ï¼Œä¸è¦æˆªæ–­
    # 4. é¿å…åœ¨å­—ç¬¦ä¸²ä¸­ä½¿ç”¨ä¸­æ–‡å¼•å·ï¼Œç»Ÿä¸€ä½¿ç”¨è‹±æ–‡åŒå¼•å·
    # 5. æ¯ä¸ªå±æ€§å€¼éƒ½è¦æ­£ç¡®é—­åˆå¼•å·
    # 6. ç¡®ä¿JSONæ ¼å¼å®Œå…¨æ­£ç¡®ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„é€—å·ã€å¼•å·å’Œé—­åˆæ‹¬å·

examples = [
    lx.data.ExampleData(
          text=(
              "å±±è¥¿äº‘æ™Ÿç§‘æŠ€æœ‰é™å…¬å¸æˆç«‹äº2015å¹´ï¼Œåè½äºæœ‰å¤ªè¡Œæ˜ç ä¹‹ç§°çš„å±±è¥¿çœæ™‹åŸå¸‚ï¼Œå¹¶å…ˆååœ¨æ­¦æ±‰å¸‚ã€è¥¿å®‰å¸‚ã€é•¿æ²»å¸‚ã€ä¸´æ±¾å¸‚ã€é˜³æ³‰å¸‚æˆç«‹å­åˆ†å…¬å¸ã€‚å…¬å¸æ³¨å†Œèµ„é‡‘1000ä¸‡ã€‚"
          ),
          extractions=[
              lx.data.Extraction(
                  extraction_class="relationship",
                  extraction_text=(
                      "å±±è¥¿äº‘æ™Ÿç§‘æŠ€æœ‰é™å…¬å¸æˆç«‹äº2015å¹´"
                  ),
                  attributes={
                      "type": "æˆç«‹æ—¶é—´",
                      "entity_1": "å±±è¥¿äº‘æ™Ÿç§‘æŠ€æœ‰é™å…¬å¸",
                      "entity_2": "2015å¹´",
                  },
              ),
              lx.data.Extraction(
                  extraction_class="relationship",
                  extraction_text=(
                      "åè½äºæœ‰å¤ªè¡Œæ˜ç ä¹‹ç§°çš„å±±è¥¿çœæ™‹åŸå¸‚"
                  ),
                  attributes={
                      "type": "æˆç«‹åœ°ç‚¹",
                      "entity_1": "å±±è¥¿äº‘æ™Ÿç§‘æŠ€æœ‰é™å…¬å¸",
                      "entity_2": "å±±è¥¿çœæ™‹åŸå¸‚",
                  },
              ),
              lx.data.Extraction(
                  extraction_class="relationship",
                  extraction_text=(
                      "å¹¶å…ˆååœ¨æ­¦æ±‰å¸‚ã€è¥¿å®‰å¸‚ã€é•¿æ²»å¸‚ã€ä¸´æ±¾å¸‚ã€é˜³æ³‰å¸‚æˆç«‹å­åˆ†å…¬å¸ã€‚"
                  ),
                  attributes={
                      "type": "åˆ†å…¬å¸åœ°ç‚¹",
                      "entity_1": "å±±è¥¿äº‘æ™Ÿç§‘æŠ€æœ‰é™å…¬å¸",
                      "entity_2": "æ­¦æ±‰å¸‚ã€è¥¿å®‰å¸‚ã€é•¿æ²»å¸‚ã€ä¸´æ±¾å¸‚ã€é˜³æ³‰å¸‚",
                  },
              ),
              lx.data.Extraction(
                  extraction_class="relationship",
                  extraction_text=(
                      "å…¬å¸æ³¨å†Œèµ„é‡‘1000ä¸‡ã€‚"
                  ),
                  attributes={
                      "type": "æ³¨å†Œèµ„é‡‘",
                      "entity_1": "å±±è¥¿äº‘æ™Ÿç§‘æŠ€æœ‰é™å…¬å¸",
                      "entity_2": "1000ä¸‡",
                  },
              ),
          ],
      )
]

def read_docx_file(file_path):
    """è¯»å– DOCX æ–‡ä»¶å†…å®¹"""
    try:
        doc = docx.Document(file_path)
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        return text.strip()
    except Exception as e:
        print(f"âŒ è¯»å– DOCX æ–‡ä»¶å¤±è´¥: {e}")
        return None


def normalize_quotes(text):
    """æ ‡å‡†åŒ–å¼•å·ï¼Œå°†æ‰€æœ‰åŒå¼•å·æ›¿æ¢ä¸ºä¸‹åˆ’çº¿"""
    # å°†æ‰€æœ‰ç±»å‹çš„åŒå¼•å·æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
    quote_mapping = {
        '"': '_',  # ä¸­æ–‡åŒå¼•å·å·¦ -> ä¸‹åˆ’çº¿
        '"': '_',  # ä¸­æ–‡åŒå¼•å·å³ -> ä¸‹åˆ’çº¿
        '"': '_',  # è‹±æ–‡åŒå¼•å· -> ä¸‹åˆ’çº¿
        ''': '_',  # ä¸­æ–‡å•å¼•å·å·¦ -> ä¸‹åˆ’çº¿
        ''': '_',  # ä¸­æ–‡å•å¼•å·å³ -> ä¸‹åˆ’çº¿
        "'": '_',  # è‹±æ–‡å•å¼•å· -> ä¸‹åˆ’çº¿
    }
    
    for quote, underscore in quote_mapping.items():
        text = text.replace(quote, underscore)
    
    return text


def run_docx_extraction(model_id="qwen3:8b", temperature=0.3, docx_path="/app/test_data/æ™‹æ§åˆ›åŠ›äº§å“ä»‹ç»æ–‡å­—ç‰ˆ.docx"):
    """ä¸“é—¨å¤„ç† DOCX æ–‡ä»¶çš„æå–å‡½æ•°"""
    print(f"ğŸ“„ æ­£åœ¨å¤„ç† DOCX æ–‡ä»¶: {docx_path}")
    
    # è¯»å– DOCX æ–‡ä»¶
    text_content = read_docx_file(docx_path)
    if text_content is None:
        raise FileNotFoundError(f"æ— æ³•è¯»å–æ–‡ä»¶: {docx_path}")
    
    # æ ‡å‡†åŒ–å¼•å·
    text_content = normalize_quotes(text_content)
    
    print(f"ğŸ“ æ–‡ä»¶å†…å®¹é•¿åº¦: {len(text_content)} å­—ç¬¦")
    print(f"ğŸ“– å†…å®¹é¢„è§ˆ: {text_content[:200]}...")
    
    # é…ç½®æ¨¡å‹
    model_config = lx.factory.ModelConfig(
        model_id=model_id,
        provider_kwargs={
            "model_url": os.getenv("OLLAMA_HOST", "http://localhost:11434"),
            "format_type": lx.data.FormatType.JSON,
            "temperature": temperature,
        },
    )
    
    try:
        # æ‰§è¡Œæå–
        result = lx.extract(
            text_or_documents=text_content,
            prompt_description=prompt,
            examples=examples,
            config=model_config,
            use_schema_constraints=True,
            extraction_passes=2,      # å‡å°‘passesä»¥é¿å…JSONé”™è¯¯ç´¯ç§¯
            max_workers=10,           # å‡å°‘å¹¶å‘ä»¥é¿å…ç«äº‰æ¡ä»¶
            max_char_buffer=500       # å‡å°ç¼“å†²åŒºä»¥æé«˜å‡†ç¡®æ€§
        )
        
        return result
        
    except Exception as e:
        print(f"âš ï¸  æå–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("ğŸ”„ å°è¯•ä½¿ç”¨æ›´ç®€å•çš„é…ç½®é‡æ–°æå–...")
        

def run_literary_extraction(model_id="qwen3:8b", temperature=0.3, text_source="gutenberg"):
    """Run literary text extraction using Ollama."""
    
    if text_source == "gutenberg":
        # Process Romeo & Juliet directly from Project Gutenberg
        print("Downloading and processing Romeo and Juliet from Project Gutenberg...")
        text_or_documents = "https://www.gutenberg.org/files/1513/1513-0.txt"
    elif text_source == "docx":
        # Use local DOCX file
        docx_path = "/app/test_data/æ™‹æ§åˆ›åŠ›äº§å“ä»‹ç»æ–‡å­—ç‰ˆ.docx"
        print(f"Processing local DOCX file: {docx_path}")
        text_content = read_docx_file(docx_path)
        if text_content is None:
            raise FileNotFoundError(f"æ— æ³•è¯»å–æ–‡ä»¶: {docx_path}")
        text_or_documents = text_content
    else:
        # Use sample text for testing
        text_or_documents = textwrap.dedent("""\
            æ™‹æ§åˆ›åŠ›ï¼ˆå±±è¥¿æ™‹æ§è£…å¤‡åˆ›åŠ›æ™ºèƒ½åˆ¶é€ æœ‰é™å…¬å¸ä»‹ç»ï¼‰æˆç«‹äº2021å¹´9æœˆ30æ—¥ï¼Œæ³¨å†Œåœ°ä½äºé•¿æ²»å¸‚ç»æµæŠ€æœ¯å¼€å‘åŒºï¼Œæ³¨å†Œèµ„æœ¬10000ä¸‡å…ƒã€‚å…¬å¸åœ¨ç…¤çŸ¿æœºæ¢°é¢†åŸŸç§¯ææ¢ç´¢æ™ºèƒ½åŒ–ã€ç»¿è‰²åŒ–è½¬å‹ï¼Œè‡´åŠ›äºæ¨åŠ¨ç…¤æœºè£…å¤‡å‡çº§å’Œåˆ¶é€ æ¨¡å¼åˆ›æ–°ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡æŠ€æœ¯åˆ›æ–°å’Œå·¥è‰ºä¼˜åŒ–ï¼Œæå‡ç…¤æœºäº§å“çš„æ™ºèƒ½åŒ–æ°´å¹³ï¼ŒåŒæ—¶é™ä½èƒ½è€—ä¸æ’æ”¾ï¼ŒåŠ©åŠ›è¡Œä¸šå¯æŒç»­å‘å±•ã€‚è¿™ä¸€æ–¹å‘æ—¢ç¬¦åˆå›½å®¶æ¨åŠ¨åˆ¶é€ ä¸šæ™ºèƒ½åŒ–ã€ç»¿è‰²åŒ–å‡çº§çš„æ”¿ç­–è¦æ±‚ï¼Œä¹Ÿæ˜¯æˆ‘ä»¬ç«‹è¶³è¡Œä¸šå®é™…ã€ç¨³æ­¥æ¨è¿›äº§ä¸šç°ä»£åŒ–çš„é‡è¦è·¯å¾„ã€‚å…¬å¸ä»¥æ‰“é€ é«˜ç«¯æ™ºèƒ½å¼€é‡‡æ§åˆ¶æŠ€æœ¯è£…å¤‡äº§å“ä¸ºä¸»ï¼Œç ”å‘ã€åˆ¶é€ ç»¼é‡‡å·¥ä½œé¢æ¶²å‹æ”¯æ¶ç”µæ¶²æ§ç³»ç»Ÿã€æ™ºèƒ½åŒ–æ§åˆ¶ç³»ç»Ÿã€é›†ä¸­ä¾›æ¶²ç³»ç»Ÿã€é«˜ç«¯æ™ºèƒ½ä¹³åŒ–æ¶²æ³µç«™ï¼Œé«˜ç«¯æ™ºèƒ½å–·é›¾æ³µç«™ã€‚""")
        print("Processing sample Chinese text...")

    model_config = lx.factory.ModelConfig(
        model_id=model_id,
        provider_kwargs={
            "model_url": os.getenv("OLLAMA_HOST", "http://localhost:11434"),
            "format_type": lx.data.FormatType.JSON,
            "temperature": temperature,
        },
    )

    result = lx.extract(
        text_or_documents=text_or_documents,
        prompt_description=prompt,
        examples=examples,
        config=model_config,
        use_schema_constraints=True,
        extraction_passes=3,      # Multiple passes for improved recall
        max_workers=20,           # Parallel processing for speed
        max_char_buffer=1000      # Smaller contexts for better accuracy
    )

    return result


def save_and_visualize_results(result, model_id, temperature, text_source, docx_path=None):
    """Save results to files and generate interactive visualization."""
    
    # ä»æ–‡æ¡£è·¯å¾„ä¸­æå–æ–‡æ¡£åç§°
    if docx_path and os.path.exists(docx_path):
        # ä»å®Œæ•´è·¯å¾„ä¸­æå–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        doc_name = os.path.splitext(os.path.basename(docx_path))[0]
    else:
        # å¦‚æœæ²¡æœ‰æ–‡æ¡£è·¯å¾„ï¼Œä½¿ç”¨text_sourceä½œä¸ºæ–‡æ¡£åç§°
        doc_name = text_source
    
    # æ¸…ç†æ–‡æ¡£åç§°ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦
    doc_name = doc_name.replace(' ', '_').replace('/', '_').replace('\\', '_')
    
    # æ¸…ç†æ¨¡å‹IDï¼Œç§»é™¤å†’å·
    clean_model_id = model_id.replace(':', '_')
    
    # ç”Ÿæˆæ–‡ä»¶åï¼š{æ–‡æ¡£åç§°}_{æ¨¡å‹ID}
    base_filename = f"{doc_name}_{clean_model_id}"
    
    # Save as JSONL format for visualization
    jsonl_file = f"{base_filename}.jsonl"
    lx.io.save_annotated_documents([result], output_name=jsonl_file, output_dir=".")
    print(f"ğŸ“Š JSONL æ–‡ä»¶å·²ä¿å­˜åˆ°: {jsonl_file}")
    
    # Generate the interactive visualization
    try:
        print("ğŸ¨ æ­£åœ¨ç”Ÿæˆäº¤äº’å¼å¯è§†åŒ–...")
        html_content = lx.visualize(jsonl_file)
        
        html_file = f"{base_filename}_visualization.html"
        with open(html_file, "w", encoding='utf-8') as f:
            if hasattr(html_content, 'data'):
                f.write(html_content.data)  # For Jupyter/Colab
            else:
                f.write(html_content)
        
        print(f"ğŸŒ äº¤äº’å¼å¯è§†åŒ–å·²ä¿å­˜åˆ°: {html_file}")
        print(f"   è¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ {html_file} æŸ¥çœ‹å¯è§†åŒ–ç»“æœ")
        
    except Exception as e:
        print(f"âš ï¸  å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
        print("   ä½† JSONL æ–‡ä»¶å·²æˆåŠŸä¿å­˜")


def print_extraction_results(result, model_id, text_source):
    """Print extraction results in a formatted way."""
    print(f"Extracted {len(result.extractions)} entities from {len(result.text):,} characters")

    if result.extractions:
        print(f"\nğŸ“ Found {len(result.extractions)} extraction(s):\n")
        
        # Group extractions by class
        extraction_counts = Counter(extraction.extraction_class for extraction in result.extractions)
        print("ğŸ“Š Extraction Summary:")
        for class_name, count in extraction_counts.most_common():
            print(f"   {class_name}: {count}")
        print()
        
        # Show first few extractions
        for i, extraction in enumerate(result.extractions[:5], 1):
            print(f"Extraction {i}:")
            print(f"  Class: {extraction.extraction_class}")
            print(f"  Text: {extraction.extraction_text[:100]}{'...' if len(extraction.extraction_text) > 100 else ''}")
            print(f"  Attributes: {extraction.attributes}")
            if extraction.char_interval:
                print(f"  Position: {extraction.char_interval.start_pos}-{extraction.char_interval.end_pos}")
            print()
        
        if len(result.extractions) > 5:
            print(f"... and {len(result.extractions) - 5} more extractions")
    else:
        print("\nâš ï¸  No extractions found")

    print("âœ… SUCCESS! Literary extraction completed")
    print(f"   Model: {model_id}")
    print(f"   Text source: {text_source}")
    print("   JSON mode: enabled")
    print("   Schema constraints: enabled")
    print("   Parallel processing: enabled")


# ç¤ºä¾‹è°ƒç”¨å‡½æ•°
def run_example():
    """ç¤ºä¾‹ï¼šè¿è¡Œ DOCX æ–‡ä»¶æå–"""
    try:
        print("ğŸš€ Running DOCX extraction with qwen3:8b...")
        print("ğŸ“„ File: /app/test_data/æ™‹æ§åˆ›åŠ›äº§å“ä»‹ç»æ–‡å­—ç‰ˆ.docx")
        print("-" * 50)
        
        # ä½¿ç”¨ä¸“é—¨çš„ DOCX å¤„ç†å‡½æ•°
        result = run_docx_extraction(
            model_id="qwen3:8b",
            temperature=0.1,
            docx_path="/app/test_data/æ™‹æ§åˆ›åŠ›äº§å“ä»‹ç»æ–‡å­—ç‰ˆ.docx"
        )
        
        # æ‰“å°ç»“æœ
        print_extraction_results(result, "qwen3:8b", "docx")
        
        # ä¿å­˜å’Œå¯è§†åŒ–ç»“æœ
        save_and_visualize_results(result, "qwen3:8b", 0.3, "docx", "/app/test_data/æ™‹æ§åˆ›åŠ›äº§å“ä»‹ç»æ–‡å­—ç‰ˆ.docx")
        
        return True
        
    except ConnectionError as e:
        print(f"\nâŒ ConnectionError: {e}")
        print("\nğŸ’¡ Make sure Ollama is running:")
        print("   ollama serve")
        return False
    except ValueError as e:
        if "Can't find Ollama" in str(e):
            print(f"\nâŒ Model not found: qwen3:8b")
            print("\nğŸ’¡ Install the model first:")
            print("   ollama pull qwen3:8b")
        else:
            print(f"\nâŒ ValueError: {e}")
        return False
    except Exception as e:
        print(f"\nâŒ Error: {type(e).__name__}: {e}")
        return False


if __name__ == "__main__":
    run_example()