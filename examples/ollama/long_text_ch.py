import langextract as lx
import textwrap
import os
from collections import Counter, defaultdict
import docx

# Define comprehensive prompt and examples for complex literary text
prompt = textwrap.dedent("""\
    æå–å‡ºå…¬å¸ã€äº§å“çš„åç§°ã€ç”¨é€”ã€æŠ€æœ¯å‚æ•°ã€ä½¿ç”¨æ–¹æ³•ã€ç‰¹ç‚¹ã€ä¼˜åŠ¿ç­‰å¯¹å…¬å¸å’Œäº§å“è¿›è¡Œä»‹ç»çš„ä¿¡æ¯ã€‚
    ä»åŸæ–‡ä¸­ä¸ºæ¯ä¸ªå®ä½“æä¾›æœ‰æ„ä¹‰çš„å±æ€§ï¼Œä»¥å¢åŠ ä¸Šä¸‹æ–‡å’Œæ·±åº¦ã€‚
    åŒç±»å®ä½“ç”±äºå…¶æè¿°æ–‡æœ¬çš„ä¸åŒï¼Œå±æ€§ç±»å¯ä»¥ä¸å®Œå…¨ç›¸åŒï¼Œä¸€äº›å±æ€§åªæœ‰æŸä¸ªç±»æœ‰ï¼Œæå–å±æ€§éœ€è¦åŸºäºåŸæ–‡æœ¬ã€‚åŒç±»å®ä½“ä¸­ï¼Œæ„ä¹‰ç›¸ä¼¼çš„å±æ€§ç±»ï¼Œéœ€è¦åŒåè§„èŒƒåŒ–ã€‚
    """)
    # é‡è¦è¦æ±‚ï¼š
    # 1. å¿…é¡»åŒ…å« "extractions" é”®ï¼Œä¸”å¿…é¡»æ˜¯æ•°ç»„
    # 2. æ‰€æœ‰å­—ç¬¦ä¸²å€¼å¿…é¡»ç”¨è‹±æ–‡åŒå¼•å·åŒ…å›´
    # 3. ç¡®ä¿JSONç»“æ„å®Œæ•´ï¼Œä¸è¦æˆªæ–­
    # 4. é¿å…åœ¨å­—ç¬¦ä¸²ä¸­ä½¿ç”¨ä¸­æ–‡å¼•å·ï¼Œç»Ÿä¸€ä½¿ç”¨è‹±æ–‡åŒå¼•å·
    # 5. æ¯ä¸ªå±æ€§å€¼éƒ½è¦æ­£ç¡®é—­åˆå¼•å·
    # 6. ç¡®ä¿JSONæ ¼å¼å®Œå…¨æ­£ç¡®ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„é€—å·ã€å¼•å·å’Œé—­åˆæ‹¬å·


examples = [
    lx.data.ExampleData(
        text=textwrap.dedent("""\
            æ™‹æ§åˆ›åŠ›ï¼ˆå±±è¥¿æ™‹æ§è£…å¤‡åˆ›åŠ›æ™ºèƒ½åˆ¶é€ æœ‰é™å…¬å¸ä»‹ç»ï¼‰æˆç«‹äº2021å¹´9æœˆ30æ—¥ï¼Œæ³¨å†Œåœ°ä½äºé•¿æ²»å¸‚ç»æµæŠ€æœ¯å¼€å‘åŒºï¼Œæ³¨å†Œèµ„æœ¬10000ä¸‡å…ƒã€‚
            FHJ12çŸ¿ç”¨æœ¬å®‰å‹é”®ç›˜ä¸ºçŸ¿ç”¨æœ¬å®‰å‹ï¼Œé€‚ç”¨äºç…¤çŸ¿  äº•ä¸‹ï¼Œé€šè¿‡æŒ‰é”®è¾“å…¥æŒ‡ä»¤å¹¶å‘é€ç»™æ§åˆ¶å™¨æ‰§è¡Œç›¸åº”åŠŸèƒ½ ,èƒ½è®©å·¥ä½œäººå‘˜åœ¨å®‰å…¨åŒºåŸŸæ§åˆ¶å’Œæ“ä½œæ§åˆ¶å™¨ã€‚
            å·¥ä½œç”µå‹ï¼š DC12V
            å·¥ä½œç”µæµï¼š â‰¤50mA
            ä¼ è¾“æ–¹å¼ï¼š RS485
            ä¼ è¾“é€Ÿç‡ï¼š 115.2Kbps
            æ¥å£æ•°é‡ï¼š 1è·¯
            æœ€å¤§ä¼ è¾“è·ç¦»ï¼š 10m
            é˜²çˆ†æ ‡å¿—ï¼šExib I Mb
            å¤–å½¢å°ºå¯¸ï¼š 185mmX82mmX30mm
            ç‰¹ç‚¹ï¼šé˜²æŠ¤ç­‰çº§IP68ï¼Œé€‚ç”¨äºæ”¾é¡¶ç…¤ æ”¯æ¶ï¼Œæ–¹ä¾¿æ”¾ç…¤çš„æ“ä½œ
            XR ç…¤çŸ¿ åå¤§å·¥ç§æŠ€èƒ½åŸ¹è®­ç³»ç»Ÿ;åå¤§å·¥ç§åˆ†åˆ«æ˜¯ï¼šç…¤çŸ¿å®‰å…¨ç›‘æµ‹ç›‘æ§ä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿é‡‡ç…¤æœºæ“ä½œä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿é˜²çªä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿äº•ä¸‹çˆ†ç ´ä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿äº•ä¸‹ç”µæ°”ä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿æ˜è¿›æœºæ“ä½œä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿æ¢æ”¾æ°´ä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿æå‡æœºæ“ä½œä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿ç“¦æ–¯æŠ½æŸ¥ä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿ç“¦æ–¯æ£€æŸ¥ä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†	
            ç³»ç»Ÿä»‹ç»ï¼šåå¤§å·¥ç§æŠ€èƒ½åŸ¹è®­ç³»ç»Ÿçš„æ•™å­¦åŠè€ƒæ ¸å†…å®¹ä¾æ®ã€Šä¸­åäººæ°‘å…±å’Œå›½å®‰å…¨ç”Ÿäº§æ³•ã€‹ã€ã€Šç…¤çŸ¿å®‰å…¨è§„ç¨‹ã€‹ã€ã€Šç‰¹ç§ä½œä¸šäººå‘˜å®‰å…¨æŠ€æœ¯åŸ¹è®­è€ƒæ ¸ç®¡ç†è§„å®šã€‹ç­‰æ³•å¾‹ã€æ³•è§„å’Œæ ‡å‡†åˆ¶å®šã€‚
            æ­¤ç³»ç»Ÿé€šè¿‡è™šæ‹Ÿç°å®æŠ€æœ¯ï¼Œè®©å­¦å‘˜æˆä¸ºæ•™å­¦ç¯èŠ‚çš„å‚ä¸è€…ï¼Œä½¿ç†è®ºå­¦ä¹ ä¸å®é™…æ“ä½œå¯åŒæ—¶è¿›è¡Œã€‚ä»¥å‡å°‘æ•™å­¦ç¯èŠ‚ï¼Œé™ä½ä¼ä¸šåŸ¹è®­æˆæœ¬ä¸ºç›®çš„ï¼Œä¸éœ€è¦ä¼ ç»Ÿçš„æ•™å­¦æ¨¡å¼ï¼Œå¤šä½å­¦å‘˜å¯åŒæ—¶è¿›è¡Œæ“ä½œç»ƒä¹ å’Œè€ƒæ ¸ã€‚è§£å†³æ•™å¸ˆçš„å·¥ä½œè´Ÿæ‹…ã€æé«˜å­¦ä¹ æ•ˆç‡ã€‚å…¶ä¼˜ç‚¹åœ¨äºæ•™å­¦åœºåœ°å ç”¨å°,å†…å®¹ç”ŸåŠ¨ä¸°å¯Œï¼Œæ•™å­¦æ•ˆæœæ˜æ˜¾ç­‰ã€‚å¯ä¸ºç…¤çŸ¿ä¼ä¸šå¤§å¹…é™ä½åŸ¹è®­æˆæœ¬,é›¶é£é™©åŸ¹è®­,æé«˜ç…¤ç‚­æ“ä½œå²—ä½å®æ“æ•™å­¦è´¨é‡,æœ€ç»ˆå®ç°å‘˜å·¥é«˜ç´ è´¨å¿«é€Ÿå…¥å²—ã€‚
            """),
        extractions=[
            # lx.data.Extraction(
            #       extraction_class="å…¬å¸ä»‹ç»",
            #       extraction_text=(
            #           "å±±è¥¿äº‘æ™Ÿç§‘æŠ€æœ‰é™å…¬å¸æˆç«‹äº2015å¹´ï¼Œåè½äºæœ‰å¤ªè¡Œæ˜ç ä¹‹ç§°çš„å±±è¥¿çœæ™‹åŸå¸‚ï¼Œå¹¶å…ˆååœ¨æ­¦æ±‰å¸‚ã€è¥¿å®‰å¸‚ã€é•¿æ²»å¸‚ã€ä¸´æ±¾å¸‚ã€é˜³æ³‰å¸‚æˆç«‹å­åˆ†å…¬å¸ã€‚å…¬å¸æ³¨å†Œèµ„é‡‘1000ä¸‡ã€‚        äº‘æ™Ÿç§‘æŠ€æ˜¯å›½å†…è§£å†³å·¥ä¸šè¡Œä¸šå®æ“åŸ¹è®­çš„ç§‘æŠ€å…¬å¸ã€‚å…¬å¸è‡´åŠ›äºå°†AI+XR(VRã€MRã€APPç­‰)é«˜å°–ç«¯æŠ€æœ¯åº”ç”¨äºå®‰å…¨æ•™è‚²ä¸åŸ¹è®­é¢†åŸŸï¼Œå¸®åŠ©ä¼ä¸šå’Œä¸ªäººæ›´é«˜æ•ˆã€å®‰å…¨ã€çœŸå®çš„ä½“éªŒã€å­¦ä¹ ã€‚     "
            #       ),
            #       attributes={
            #           "name": "å±±è¥¿äº‘æ™Ÿç§‘æŠ€æœ‰é™å…¬å¸",
            #           "genre": "äº‘æ™Ÿç§‘æŠ€æ˜¯å›½å†…è§£å†³å·¥ä¸šè¡Œä¸šå®æ“åŸ¹è®­çš„ç§‘æŠ€å…¬å¸",
            #       },
            # ),
            lx.data.Extraction(
                extraction_class="äº§å“",
                extraction_text=(
                    """
                    FHJ12çŸ¿ç”¨æœ¬å®‰å‹é”®ç›˜ä¸ºçŸ¿ç”¨æœ¬å®‰å‹ï¼Œé€‚ç”¨äºç…¤çŸ¿  äº•ä¸‹ï¼Œé€šè¿‡æŒ‰é”®è¾“å…¥æŒ‡ä»¤å¹¶å‘é€ç»™æ§åˆ¶å™¨æ‰§è¡Œç›¸åº”åŠŸèƒ½ ,èƒ½è®©å·¥ä½œäººå‘˜åœ¨å®‰å…¨åŒºåŸŸæ§åˆ¶å’Œæ“ä½œæ§åˆ¶å™¨ã€‚
                    å·¥ä½œç”µå‹ï¼š DC12V
                    å·¥ä½œç”µæµï¼š â‰¤50mA
                    ä¼ è¾“æ–¹å¼ï¼š RS485
                    ä¼ è¾“é€Ÿç‡ï¼š 115.2Kbps
                    æ¥å£æ•°é‡ï¼š 1è·¯
                    æœ€å¤§ä¼ è¾“è·ç¦»ï¼š 10m
                    é˜²çˆ†æ ‡å¿—ï¼šExib I Mb
                    å¤–å½¢å°ºå¯¸ï¼š 185mmX82mmX30mm
                    ç‰¹ç‚¹ï¼šé˜²æŠ¤ç­‰çº§IP68ï¼Œé€‚ç”¨äºæ”¾é¡¶ç…¤ æ”¯æ¶ï¼Œæ–¹ä¾¿æ”¾ç…¤çš„æ“ä½œ
                    """
                ),
                attributes={
                    "product_name": "FHJ12çŸ¿ç”¨æœ¬å®‰å‹é”®ç›˜",
                    "type": "çŸ¿ç”¨æœ¬å®‰å‹",
                    "usage": "é€šè¿‡æŒ‰é”®è¾“å…¥æŒ‡ä»¤å¹¶å‘é€ç»™æ§åˆ¶å™¨æ‰§è¡Œç›¸åº”åŠŸèƒ½ï¼Œèƒ½è®©å·¥ä½œäººå‘˜åœ¨å®‰å…¨åŒºåŸŸæ§åˆ¶å’Œæ“ä½œæ§åˆ¶å™¨",
                    "working_voltage": "DC12V",
                    "working_current": "â‰¤50mA",
                    "transmission_method": "RS485",
                    "transmission_rate": "115.2Kbps",
                    "interface_count": "1è·¯",
                    "max_transmission_distance": "10m",
                    "explosion_proof_mark": "Exib I Mb",
                    "dimensions": "185mmX82mmX30mm",
                    "features": "é˜²æŠ¤ç­‰çº§IP68ï¼Œé€‚ç”¨äºæ”¾é¡¶ç…¤æ”¯æ¶ï¼Œæ–¹ä¾¿æ”¾ç…¤çš„æ“ä½œ"
                },
            ),
            lx.data.Extraction(
                extraction_class="å…¬å¸",
                extraction_text=(
                    """
                    æ™‹æ§åˆ›åŠ›ï¼ˆå±±è¥¿æ™‹æ§è£…å¤‡åˆ›åŠ›æ™ºèƒ½åˆ¶é€ æœ‰é™å…¬å¸ä»‹ç»ï¼‰æˆç«‹äº2021å¹´9æœˆ30æ—¥ï¼Œæ³¨å†Œåœ°ä½äºé•¿æ²»å¸‚ç»æµæŠ€æœ¯å¼€å‘åŒºï¼Œæ³¨å†Œèµ„æœ¬10000ä¸‡å…ƒã€‚
                    """
                ),
                attributes={
                    "establishment_date": "2021å¹´9æœˆ30æ—¥",
                    "location": "é•¿æ²»å¸‚ç»æµæŠ€æœ¯å¼€å‘åŒº",
                    "registered_capital": "10000ä¸‡å…ƒ",
                }
            ),
            lx.data.Extraction(
                extraction_class="äº§å“",
                extraction_text=(
                    """
                    XR ç…¤çŸ¿ åå¤§å·¥ç§æŠ€èƒ½åŸ¹è®­ç³»ç»Ÿ;åå¤§å·¥ç§åˆ†åˆ«æ˜¯ï¼šç…¤çŸ¿å®‰å…¨ç›‘æµ‹ç›‘æ§ä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿é‡‡ç…¤æœºæ“ä½œä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿é˜²çªä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿äº•ä¸‹çˆ†ç ´ä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿äº•ä¸‹ç”µæ°”ä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿æ˜è¿›æœºæ“ä½œä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿æ¢æ”¾æ°´ä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿æå‡æœºæ“ä½œä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿ç“¦æ–¯æŠ½æŸ¥ä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿ç“¦æ–¯æ£€æŸ¥ä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†	
                    ç³»ç»Ÿä»‹ç»ï¼šåå¤§å·¥ç§æŠ€èƒ½åŸ¹è®­ç³»ç»Ÿçš„æ•™å­¦åŠè€ƒæ ¸å†…å®¹ä¾æ®ã€Šä¸­åäººæ°‘å…±å’Œå›½å®‰å…¨ç”Ÿäº§æ³•ã€‹ã€ã€Šç…¤çŸ¿å®‰å…¨è§„ç¨‹ã€‹ã€ã€Šç‰¹ç§ä½œä¸šäººå‘˜å®‰å…¨æŠ€æœ¯åŸ¹è®­è€ƒæ ¸ç®¡ç†è§„å®šã€‹ç­‰æ³•å¾‹ã€æ³•è§„å’Œæ ‡å‡†åˆ¶å®šã€‚
                    æ­¤ç³»ç»Ÿé€šè¿‡è™šæ‹Ÿç°å®æŠ€æœ¯ï¼Œè®©å­¦å‘˜æˆä¸ºæ•™å­¦ç¯èŠ‚çš„å‚ä¸è€…ï¼Œä½¿ç†è®ºå­¦ä¹ ä¸å®é™…æ“ä½œå¯åŒæ—¶è¿›è¡Œã€‚ä»¥å‡å°‘æ•™å­¦ç¯èŠ‚ï¼Œé™ä½ä¼ä¸šåŸ¹è®­æˆæœ¬ä¸ºç›®çš„ï¼Œä¸éœ€è¦ä¼ ç»Ÿçš„æ•™å­¦æ¨¡å¼ï¼Œå¤šä½å­¦å‘˜å¯åŒæ—¶è¿›è¡Œæ“ä½œç»ƒä¹ å’Œè€ƒæ ¸ã€‚è§£å†³æ•™å¸ˆçš„å·¥ä½œè´Ÿæ‹…ã€æé«˜å­¦ä¹ æ•ˆç‡ã€‚å…¶ä¼˜ç‚¹åœ¨äºæ•™å­¦åœºåœ°å ç”¨å°,å†…å®¹ç”ŸåŠ¨ä¸°å¯Œï¼Œæ•™å­¦æ•ˆæœæ˜æ˜¾ç­‰ã€‚å¯ä¸ºç…¤çŸ¿ä¼ä¸šå¤§å¹…é™ä½åŸ¹è®­æˆæœ¬,é›¶é£é™©åŸ¹è®­,æé«˜ç…¤ç‚­æ“ä½œå²—ä½å®æ“æ•™å­¦è´¨é‡,æœ€ç»ˆå®ç°å‘˜å·¥é«˜ç´ è´¨å¿«é€Ÿå…¥å²—ã€‚
                    """
                ),
                attributes={
                    "product_name": "XR ç…¤çŸ¿ åå¤§å·¥ç§ æŠ€èƒ½åŸ¹è®­ç³»ç»Ÿ",
                    "product_content": "åå¤§å·¥ç§åˆ†åˆ«æ˜¯ï¼šç…¤çŸ¿å®‰å…¨ç›‘æµ‹ç›‘æ§ä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿é‡‡ç…¤æœºæ“ä½œä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿é˜²çªä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿äº•ä¸‹çˆ†ç ´ä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿äº•ä¸‹ç”µæ°”ä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿æ˜è¿›æœºæ“ä½œä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿æ¢æ”¾æ°´ä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿æå‡æœºæ“ä½œä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿ç“¦æ–¯æŠ½æŸ¥ä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†ã€ç…¤çŸ¿ç“¦æ–¯æ£€æŸ¥ä½œä¸šå®‰å…¨æŠ€æœ¯å®é™…æ“ä½œè€ƒè¯•æ ‡å‡†",
                    "reference_standards": "ã€Šä¸­åäººæ°‘å…±å’Œå›½å®‰å…¨ç”Ÿäº§æ³•ã€‹ã€ã€Šç…¤çŸ¿å®‰å…¨è§„ç¨‹ã€‹ã€ã€Šç‰¹ç§ä½œä¸šäººå‘˜å®‰å…¨æŠ€æœ¯åŸ¹è®­è€ƒæ ¸ç®¡ç†è§„å®šã€‹ç­‰æ³•å¾‹ã€æ³•è§„å’Œæ ‡å‡†",
                    "product_features": "å­¦å‘˜å‚ä¸æ•™å­¦ç¯èŠ‚ï¼Œç†è®ºå­¦ä¹ ä¸å®é™…æ“ä½œå¯åŒæ—¶è¿›è¡Œï¼›å¤šä½å­¦å‘˜å¯åŒæ—¶è¿›è¡Œæ“ä½œç»ƒä¹ å’Œè€ƒæ ¸ï¼Œæ— éœ€ä¼ ç»Ÿæ•™å­¦æ¨¡å¼",
                    "product_advantages": "å‡å°‘æ•™å­¦ç¯èŠ‚ï¼Œé™ä½ä¼ä¸šåŸ¹è®­æˆæœ¬ï¼›å‡è½»æ•™å¸ˆå·¥ä½œè´Ÿæ‹…ï¼Œæé«˜å­¦ä¹ æ•ˆç‡ï¼›æ•™å­¦åœºåœ°å ç”¨å°ï¼Œå†…å®¹ç”ŸåŠ¨ä¸°å¯Œï¼Œæ•™å­¦æ•ˆæœæ˜æ˜¾ï¼›å®ç°AIæ•°å­—äººé›¶é£é™©åŸ¹è®­ï¼Œæé«˜ç…¤ç‚­æ“ä½œå²—ä½å®æ“æ•™å­¦è´¨é‡ï¼ŒåŠ©åŠ›å‘˜å·¥é«˜ç´ è´¨å¿«é€Ÿå…¥å²—"
                },
            ),
        ]
    )
]

def read_docx_file(file_path):
    """è¯»å– DOCX æ–‡ä»¶å†…å®¹"""
    try:
        doc = docx.Document(file_path)
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        return text.strip()
    except Exception as e:
        print(f"âŒ è¯»å– DOCX æ–‡ä»¶å¤±è´¥: {e}")
        return None


def normalize_quotes(text):
    """æ ‡å‡†åŒ–å¼•å·ï¼Œå°†æ‰€æœ‰åŒå¼•å·æ›¿æ¢ä¸ºä¸‹åˆ’çº¿"""
    # å°†æ‰€æœ‰ç±»å‹çš„åŒå¼•å·æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
    quote_mapping = {
        '"': '_',  # ä¸­æ–‡åŒå¼•å·å·¦ -> ä¸‹åˆ’çº¿
        '"': '_',  # ä¸­æ–‡åŒå¼•å·å³ -> ä¸‹åˆ’çº¿
        '"': '_',  # è‹±æ–‡åŒå¼•å· -> ä¸‹åˆ’çº¿
        ''': '_',  # ä¸­æ–‡å•å¼•å·å·¦ -> ä¸‹åˆ’çº¿
        ''': '_',  # ä¸­æ–‡å•å¼•å·å³ -> ä¸‹åˆ’çº¿
        "'": '_',  # è‹±æ–‡å•å¼•å· -> ä¸‹åˆ’çº¿
    }
    
    for quote, underscore in quote_mapping.items():
        text = text.replace(quote, underscore)
    
    return text


def run_docx_extraction(model_id="qwen3:8b", temperature=0.3, docx_path="/app/test_data/AIæ•°å­—äºº.docx"):
    """ä¸“é—¨å¤„ç† DOCX æ–‡ä»¶çš„æå–å‡½æ•°"""
    print(f"ğŸ“„ æ­£åœ¨å¤„ç† DOCX æ–‡ä»¶: {docx_path}")
    
    # è¯»å– DOCX æ–‡ä»¶
    text_content = read_docx_file(docx_path)
    if text_content is None:
        raise FileNotFoundError(f"æ— æ³•è¯»å–æ–‡ä»¶: {docx_path}")
    
    # æ ‡å‡†åŒ–å¼•å·
    text_content = normalize_quotes(text_content)
    
    print(f"ğŸ“ æ–‡ä»¶å†…å®¹é•¿åº¦: {len(text_content)} å­—ç¬¦")
    print(f"ğŸ“– å†…å®¹é¢„è§ˆ: {text_content[:200]}...")
    
    # é…ç½®æ¨¡å‹
    model_config = lx.factory.ModelConfig(
        model_id=model_id,
        provider_kwargs={
            "model_url": os.getenv("OLLAMA_HOST", "http://localhost:11434"),
            "format_type": lx.data.FormatType.JSON,
            "temperature": temperature,
        },
    )
    
    try:
        # æ‰§è¡Œæå–
        result = lx.extract(
            text_or_documents=text_content,
            prompt_description=prompt,
            examples=examples,
            config=model_config,
            use_schema_constraints=True,
            extraction_passes=2,      # å‡å°‘passesä»¥é¿å…JSONé”™è¯¯ç´¯ç§¯
            max_workers=10,           # å‡å°‘å¹¶å‘ä»¥é¿å…ç«äº‰æ¡ä»¶
            max_char_buffer=500       # å‡å°ç¼“å†²åŒºä»¥æé«˜å‡†ç¡®æ€§
        )
        
        return result
        
    except Exception as e:
        print(f"âš ï¸  æå–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("ğŸ”„ å°è¯•ä½¿ç”¨æ›´ç®€å•çš„é…ç½®é‡æ–°æå–...")
        

def run_literary_extraction(model_id="qwen3:8b", temperature=0.3, text_source="gutenberg"):
    """Run literary text extraction using Ollama."""
    
    if text_source == "gutenberg":
        # Process Romeo & Juliet directly from Project Gutenberg
        print("Downloading and processing Romeo and Juliet from Project Gutenberg...")
        text_or_documents = "https://www.gutenberg.org/files/1513/1513-0.txt"
    elif text_source == "docx":
        # Use local DOCX file
        docx_path = "/app/test_data/AIæ•°å­—äºº.docx"
        print(f"Processing local DOCX file: {docx_path}")
        text_content = read_docx_file(docx_path)
        if text_content is None:
            raise FileNotFoundError(f"æ— æ³•è¯»å–æ–‡ä»¶: {docx_path}")
        text_or_documents = text_content
    else:
        # Use sample text for testing
        text_or_documents = textwrap.dedent("""\
            æ™‹æ§åˆ›åŠ›ï¼ˆå±±è¥¿æ™‹æ§è£…å¤‡åˆ›åŠ›æ™ºèƒ½åˆ¶é€ æœ‰é™å…¬å¸ä»‹ç»ï¼‰æˆç«‹äº2021å¹´9æœˆ30æ—¥ï¼Œæ³¨å†Œåœ°ä½äºé•¿æ²»å¸‚ç»æµæŠ€æœ¯å¼€å‘åŒºï¼Œæ³¨å†Œèµ„æœ¬10000ä¸‡å…ƒã€‚å…¬å¸åœ¨ç…¤çŸ¿æœºæ¢°é¢†åŸŸç§¯ææ¢ç´¢æ™ºèƒ½åŒ–ã€ç»¿è‰²åŒ–è½¬å‹ï¼Œè‡´åŠ›äºæ¨åŠ¨ç…¤æœºè£…å¤‡å‡çº§å’Œåˆ¶é€ æ¨¡å¼åˆ›æ–°ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡æŠ€æœ¯åˆ›æ–°å’Œå·¥è‰ºä¼˜åŒ–ï¼Œæå‡ç…¤æœºäº§å“çš„æ™ºèƒ½åŒ–æ°´å¹³ï¼ŒåŒæ—¶é™ä½èƒ½è€—ä¸æ’æ”¾ï¼ŒåŠ©åŠ›è¡Œä¸šå¯æŒç»­å‘å±•ã€‚è¿™ä¸€æ–¹å‘æ—¢ç¬¦åˆå›½å®¶æ¨åŠ¨åˆ¶é€ ä¸šæ™ºèƒ½åŒ–ã€ç»¿è‰²åŒ–å‡çº§çš„æ”¿ç­–è¦æ±‚ï¼Œä¹Ÿæ˜¯æˆ‘ä»¬ç«‹è¶³è¡Œä¸šå®é™…ã€ç¨³æ­¥æ¨è¿›äº§ä¸šç°ä»£åŒ–çš„é‡è¦è·¯å¾„ã€‚å…¬å¸ä»¥æ‰“é€ é«˜ç«¯æ™ºèƒ½å¼€é‡‡æ§åˆ¶æŠ€æœ¯è£…å¤‡äº§å“ä¸ºä¸»ï¼Œç ”å‘ã€åˆ¶é€ ç»¼é‡‡å·¥ä½œé¢æ¶²å‹æ”¯æ¶ç”µæ¶²æ§ç³»ç»Ÿã€æ™ºèƒ½åŒ–æ§åˆ¶ç³»ç»Ÿã€é›†ä¸­ä¾›æ¶²ç³»ç»Ÿã€é«˜ç«¯æ™ºèƒ½ä¹³åŒ–æ¶²æ³µç«™ï¼Œé«˜ç«¯æ™ºèƒ½å–·é›¾æ³µç«™ã€‚""")
        print("Processing sample Chinese text...")

    model_config = lx.factory.ModelConfig(
        model_id=model_id,
        provider_kwargs={
            "model_url": os.getenv("OLLAMA_HOST", "http://localhost:11434"),
            "format_type": lx.data.FormatType.JSON,
            "temperature": temperature,
        },
    )

    result = lx.extract(
        text_or_documents=text_or_documents,
        prompt_description=prompt,
        examples=examples,
        config=model_config,
        use_schema_constraints=True,
        extraction_passes=3,      # Multiple passes for improved recall
        max_workers=20,           # Parallel processing for speed
        max_char_buffer=1000      # Smaller contexts for better accuracy
    )

    return result


def save_and_visualize_results(result, model_id, temperature, text_source, docx_path=None):
    """Save results to files and generate interactive visualization."""
    
    # ä»æ–‡æ¡£è·¯å¾„ä¸­æå–æ–‡æ¡£åç§°
    if docx_path and os.path.exists(docx_path):
        # ä»å®Œæ•´è·¯å¾„ä¸­æå–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        doc_name = os.path.splitext(os.path.basename(docx_path))[0]
    else:
        # å¦‚æœæ²¡æœ‰æ–‡æ¡£è·¯å¾„ï¼Œä½¿ç”¨text_sourceä½œä¸ºæ–‡æ¡£åç§°
        doc_name = text_source
    
    # æ¸…ç†æ–‡æ¡£åç§°ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦
    doc_name = doc_name.replace(' ', '_').replace('/', '_').replace('\\', '_')
    
    # æ¸…ç†æ¨¡å‹IDï¼Œç§»é™¤å†’å·
    clean_model_id = model_id.replace(':', '_')
    
    # ç”Ÿæˆæ–‡ä»¶åï¼š{æ–‡æ¡£åç§°}_{æ¨¡å‹ID}
    base_filename = f"{doc_name}_{clean_model_id}"
    
    # Save as JSONL format for visualization
    jsonl_file = f"{base_filename}.jsonl"
    lx.io.save_annotated_documents([result], output_name=jsonl_file, output_dir=".")
    print(f"ğŸ“Š JSONL æ–‡ä»¶å·²ä¿å­˜åˆ°: {jsonl_file}")
    
    # Generate the interactive visualization
    try:
        print("ğŸ¨ æ­£åœ¨ç”Ÿæˆäº¤äº’å¼å¯è§†åŒ–...")
        html_content = lx.visualize(jsonl_file)
        
        html_file = f"{base_filename}_visualization.html"
        with open(html_file, "w", encoding='utf-8') as f:
            if hasattr(html_content, 'data'):
                f.write(html_content.data)  # For Jupyter/Colab
            else:
                f.write(html_content)
        
        print(f"ğŸŒ äº¤äº’å¼å¯è§†åŒ–å·²ä¿å­˜åˆ°: {html_file}")
        print(f"   è¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ {html_file} æŸ¥çœ‹å¯è§†åŒ–ç»“æœ")
        
    except Exception as e:
        print(f"âš ï¸  å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
        print("   ä½† JSONL æ–‡ä»¶å·²æˆåŠŸä¿å­˜")


def print_extraction_results(result, model_id, text_source):
    """Print extraction results in a formatted way."""
    print(f"Extracted {len(result.extractions)} entities from {len(result.text):,} characters")

    if result.extractions:
        print(f"\nğŸ“ Found {len(result.extractions)} extraction(s):\n")
        
        # Group extractions by class
        extraction_counts = Counter(extraction.extraction_class for extraction in result.extractions)
        print("ğŸ“Š Extraction Summary:")
        for class_name, count in extraction_counts.most_common():
            print(f"   {class_name}: {count}")
        print()
        
        # Show first few extractions
        for i, extraction in enumerate(result.extractions[:5], 1):
            print(f"Extraction {i}:")
            print(f"  Class: {extraction.extraction_class}")
            print(f"  Text: {extraction.extraction_text[:100]}{'...' if len(extraction.extraction_text) > 100 else ''}")
            print(f"  Attributes: {extraction.attributes}")
            if extraction.char_interval:
                print(f"  Position: {extraction.char_interval.start_pos}-{extraction.char_interval.end_pos}")
            print()
        
        if len(result.extractions) > 5:
            print(f"... and {len(result.extractions) - 5} more extractions")
    else:
        print("\nâš ï¸  No extractions found")

    print("âœ… SUCCESS! Literary extraction completed")
    print(f"   Model: {model_id}")
    print(f"   Text source: {text_source}")
    print("   JSON mode: enabled")
    print("   Schema constraints: enabled")
    print("   Parallel processing: enabled")


# ç¤ºä¾‹è°ƒç”¨å‡½æ•°
def run_example():
    """ç¤ºä¾‹ï¼šè¿è¡Œ DOCX æ–‡ä»¶æå–"""
    try:
        print("ğŸš€ Running DOCX extraction with qwen3:8b...")
        print("ğŸ“„ File: /app/test_data/AIæ•°å­—äºº.docx")
        print("-" * 50)
        
        # ä½¿ç”¨ä¸“é—¨çš„ DOCX å¤„ç†å‡½æ•°
        result = run_docx_extraction(
            model_id="qwen3:8b",
            temperature=0.1,
            docx_path="/app/test_data/AIæ•°å­—äºº.docx"
        )
        
        # æ‰“å°ç»“æœ
        print_extraction_results(result, "qwen3:8b", "docx")
        
        # ä¿å­˜å’Œå¯è§†åŒ–ç»“æœ
        save_and_visualize_results(result, "qwen3:8b", 0.3, "docx", "/app/test_data/AIæ•°å­—äºº.docx")
        
        return True
        
    except ConnectionError as e:
        print(f"\nâŒ ConnectionError: {e}")
        print("\nğŸ’¡ Make sure Ollama is running:")
        print("   ollama serve")
        return False
    except ValueError as e:
        if "Can't find Ollama" in str(e):
            print(f"\nâŒ Model not found: qwen3:8b")
            print("\nğŸ’¡ Install the model first:")
            print("   ollama pull qwen3:8b")
        else:
            print(f"\nâŒ ValueError: {e}")
        return False
    except Exception as e:
        print(f"\nâŒ Error: {type(e).__name__}: {e}")
        return False


if __name__ == "__main__":
    run_example()