import langextract as lx
import textwrap
import os
from collections import Counter, defaultdict
import docx

# Define comprehensive prompt and examples for complex literary text
prompt = textwrap.dedent("""\
    æå–å‡ºå…¬å¸ã€äº§å“çš„åç§°ã€ç”¨é€”ã€æŠ€æœ¯å‚æ•°ã€ä½¿ç”¨æ–¹æ³•ã€ç‰¹ç‚¹ã€ä¼˜åŠ¿ç­‰å¯¹å…¬å¸å’Œäº§å“è¿›è¡Œä»‹ç»çš„ä¿¡æ¯ã€‚
    ä»åŸæ–‡ä¸­ä¸ºæ¯ä¸ªå®ä½“æä¾›æœ‰æ„ä¹‰çš„å±æ€§ï¼Œä»¥å¢åŠ ä¸Šä¸‹æ–‡å’Œæ·±åº¦ã€‚
    åŒç±»å®ä½“ç”±äºå…¶æè¿°æ–‡æœ¬çš„ä¸åŒï¼Œå±æ€§ç±»å¯ä»¥ä¸å®Œå…¨ç›¸åŒï¼Œä¸€äº›å±æ€§åªæœ‰æŸä¸ªç±»æœ‰ï¼Œæå–å±æ€§éœ€è¦åŸºäºåŸæ–‡æœ¬ã€‚åŒç±»å®ä½“ä¸­ï¼Œæ„ä¹‰ç›¸ä¼¼çš„å±æ€§ç±»ï¼Œéœ€è¦åŒåè§„èŒƒåŒ–ã€‚


    é‡è¦è¦æ±‚ï¼š
    1. å¿…é¡»åŒ…å« "extractions" é”®ï¼Œä¸”å¿…é¡»æ˜¯æ•°ç»„
    2. æ‰€æœ‰å­—ç¬¦ä¸²å€¼å¿…é¡»ç”¨è‹±æ–‡åŒå¼•å·åŒ…å›´
    3. ç¡®ä¿JSONç»“æ„å®Œæ•´ï¼Œä¸è¦æˆªæ–­
    4. é¿å…åœ¨å­—ç¬¦ä¸²ä¸­ä½¿ç”¨ä¸­æ–‡å¼•å·ï¼Œç»Ÿä¸€ä½¿ç”¨è‹±æ–‡åŒå¼•å·
    5. æ¯ä¸ªå±æ€§å€¼éƒ½è¦æ­£ç¡®é—­åˆå¼•å·
    6. ç¡®ä¿JSONæ ¼å¼å®Œå…¨æ­£ç¡®ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„é€—å·ã€å¼•å·å’Œé—­åˆæ‹¬å·
    """)

examples = [
    lx.data.ExampleData(
        text=textwrap.dedent("""\
            æ™‹æ§åˆ›åŠ›ï¼ˆå±±è¥¿æ™‹æ§è£…å¤‡åˆ›åŠ›æ™ºèƒ½åˆ¶é€ æœ‰é™å…¬å¸ä»‹ç»ï¼‰æˆç«‹äº2021å¹´9æœˆ30æ—¥ï¼Œæ³¨å†Œåœ°ä½äºé•¿æ²»å¸‚ç»æµæŠ€æœ¯å¼€å‘åŒºï¼Œæ³¨å†Œèµ„æœ¬10000ä¸‡å…ƒã€‚å…¬å¸åœ¨ç…¤çŸ¿æœºæ¢°é¢†åŸŸç§¯ææ¢ç´¢æ™ºèƒ½åŒ–ã€ç»¿è‰²åŒ–è½¬å‹ï¼Œè‡´åŠ›äºæ¨åŠ¨ç…¤æœºè£…å¤‡å‡çº§å’Œåˆ¶é€ æ¨¡å¼åˆ›æ–°ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡æŠ€æœ¯åˆ›æ–°å’Œå·¥è‰ºä¼˜åŒ–ï¼Œæå‡ç…¤æœºäº§å“çš„æ™ºèƒ½åŒ–æ°´å¹³ï¼ŒåŒæ—¶é™ä½èƒ½è€—ä¸æ’æ”¾ï¼ŒåŠ©åŠ›è¡Œä¸šå¯æŒç»­å‘å±•ã€‚è¿™ä¸€æ–¹å‘æ—¢ç¬¦åˆå›½å®¶æ¨åŠ¨åˆ¶é€ ä¸šæ™ºèƒ½åŒ–ã€ç»¿è‰²åŒ–å‡çº§çš„æ”¿ç­–è¦æ±‚ï¼Œä¹Ÿæ˜¯æˆ‘ä»¬ç«‹è¶³è¡Œä¸šå®é™…ã€ç¨³æ­¥æ¨è¿›äº§ä¸šç°ä»£åŒ–çš„é‡è¦è·¯å¾„ã€‚å…¬å¸ä»¥æ‰“é€ é«˜ç«¯æ™ºèƒ½å¼€é‡‡æ§åˆ¶æŠ€æœ¯è£…å¤‡äº§å“ä¸ºä¸»ï¼Œç ”å‘ã€åˆ¶é€ ç»¼é‡‡å·¥ä½œé¢æ¶²å‹æ”¯æ¶ç”µæ¶²æ§ç³»ç»Ÿã€æ™ºèƒ½åŒ–æ§åˆ¶ç³»ç»Ÿã€é›†ä¸­ä¾›æ¶²ç³»ç»Ÿã€é«˜ç«¯æ™ºèƒ½ä¹³åŒ–æ¶²æ³µç«™ï¼Œé«˜ç«¯æ™ºèƒ½å–·é›¾æ³µç«™ã€‚
            FHJ12çŸ¿ç”¨æœ¬å®‰å‹é”®ç›˜ä¸ºçŸ¿ç”¨æœ¬å®‰å‹ï¼Œé€‚ç”¨äºç…¤çŸ¿  äº•ä¸‹ï¼Œé€šè¿‡æŒ‰é”®è¾“å…¥æŒ‡ä»¤å¹¶å‘é€ç»™æ§åˆ¶å™¨æ‰§è¡Œç›¸åº”åŠŸèƒ½ ,èƒ½è®©å·¥ä½œäººå‘˜åœ¨å®‰å…¨åŒºåŸŸæ§åˆ¶å’Œæ“ä½œæ§åˆ¶å™¨ã€‚
            å·¥ä½œç”µå‹ï¼š DC12V
            å·¥ä½œç”µæµï¼š â‰¤50mA
            ä¼ è¾“æ–¹å¼ï¼š RS485
            ä¼ è¾“é€Ÿç‡ï¼š 115.2Kbps
            æ¥å£æ•°é‡ï¼š 1è·¯
            æœ€å¤§ä¼ è¾“è·ç¦»ï¼š 10m
            é˜²çˆ†æ ‡å¿—ï¼šExib I Mb
            å¤–å½¢å°ºå¯¸ï¼š 185mmX82mmX30mm
            ç‰¹ç‚¹ï¼šé˜²æŠ¤ç­‰çº§IP68ï¼Œé€‚ç”¨äºæ”¾é¡¶ç…¤ æ”¯æ¶ï¼Œæ–¹ä¾¿æ”¾ç…¤çš„æ“ä½œ
            """),
        extractions=[
            lx.data.Extraction(
                extraction_class="äº§å“",
                extraction_text=(
                    """
                    FHJ12çŸ¿ç”¨æœ¬å®‰å‹é”®ç›˜ä¸ºçŸ¿ç”¨æœ¬å®‰å‹ï¼Œé€‚ç”¨äºç…¤çŸ¿  äº•ä¸‹ï¼Œé€šè¿‡æŒ‰é”®è¾“å…¥æŒ‡ä»¤å¹¶å‘é€ç»™æ§åˆ¶å™¨æ‰§è¡Œç›¸åº”åŠŸèƒ½ ,èƒ½è®©å·¥ä½œäººå‘˜åœ¨å®‰å…¨åŒºåŸŸæ§åˆ¶å’Œæ“ä½œæ§åˆ¶å™¨ã€‚
                    å·¥ä½œç”µå‹ï¼š DC12V
                    å·¥ä½œç”µæµï¼š â‰¤50mA
                    ä¼ è¾“æ–¹å¼ï¼š RS485
                    ä¼ è¾“é€Ÿç‡ï¼š 115.2Kbps
                    æ¥å£æ•°é‡ï¼š 1è·¯
                    æœ€å¤§ä¼ è¾“è·ç¦»ï¼š 10m
                    é˜²çˆ†æ ‡å¿—ï¼šExib I Mb
                    å¤–å½¢å°ºå¯¸ï¼š 185mmX82mmX30mm
                    ç‰¹ç‚¹ï¼šé˜²æŠ¤ç­‰çº§IP68ï¼Œé€‚ç”¨äºæ”¾é¡¶ç…¤ æ”¯æ¶ï¼Œæ–¹ä¾¿æ”¾ç…¤çš„æ“ä½œ
                    """
                ),
                attributes={
                    "ç±»å‹": "çŸ¿ç”¨æœ¬å®‰å‹",
                    "ç”¨é€”": "é€šè¿‡æŒ‰é”®è¾“å…¥æŒ‡ä»¤å¹¶å‘é€ç»™æ§åˆ¶å™¨æ‰§è¡Œç›¸åº”åŠŸèƒ½ï¼Œèƒ½è®©å·¥ä½œäººå‘˜åœ¨å®‰å…¨åŒºåŸŸæ§åˆ¶å’Œæ“ä½œæ§åˆ¶å™¨",
                    "å·¥ä½œç”µå‹": "DC12V",
                    "å·¥ä½œç”µæµ": "â‰¤50mA",
                    "ä¼ è¾“æ–¹å¼": "RS485",
                    "ä¼ è¾“é€Ÿç‡": "115.2Kbps",
                    "æ¥å£æ•°é‡": "1è·¯",
                    "æœ€å¤§ä¼ è¾“è·ç¦»": "10m",
                    "é˜²çˆ†æ ‡å¿—": "Exib I Mb",
                    "å¤–å½¢å°ºå¯¸": "185mmX82mmX30mm",
                    "ç‰¹ç‚¹": "é˜²æŠ¤ç­‰çº§IP68ï¼Œé€‚ç”¨äºæ”¾é¡¶ç…¤æ”¯æ¶ï¼Œæ–¹ä¾¿æ”¾ç…¤çš„æ“ä½œ"
                },
            ),
            lx.data.Extraction(
                extraction_class="å…¬å¸",
                extraction_text=(
                    """
                    æ™‹æ§åˆ›åŠ›ï¼ˆå±±è¥¿æ™‹æ§è£…å¤‡åˆ›åŠ›æ™ºèƒ½åˆ¶é€ æœ‰é™å…¬å¸ä»‹ç»ï¼‰æˆç«‹äº2021å¹´9æœˆ30æ—¥ï¼Œæ³¨å†Œåœ°ä½äºé•¿æ²»å¸‚ç»æµæŠ€æœ¯å¼€å‘åŒºï¼Œæ³¨å†Œèµ„æœ¬10000ä¸‡å…ƒã€‚å…¬å¸åœ¨ç…¤çŸ¿æœºæ¢°é¢†åŸŸç§¯ææ¢ç´¢æ™ºèƒ½åŒ–ã€ç»¿è‰²åŒ–è½¬å‹ï¼Œè‡´åŠ›äºæ¨åŠ¨ç…¤æœºè£…å¤‡å‡çº§å’Œåˆ¶é€ æ¨¡å¼åˆ›æ–°ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡æŠ€æœ¯åˆ›æ–°å’Œå·¥è‰ºä¼˜åŒ–ï¼Œæå‡ç…¤æœºäº§å“çš„æ™ºèƒ½åŒ–æ°´å¹³ï¼ŒåŒæ—¶é™ä½èƒ½è€—ä¸æ’æ”¾ï¼ŒåŠ©åŠ›è¡Œä¸šå¯æŒç»­å‘å±•ã€‚è¿™ä¸€æ–¹å‘æ—¢ç¬¦åˆå›½å®¶æ¨åŠ¨åˆ¶é€ ä¸šæ™ºèƒ½åŒ–ã€ç»¿è‰²åŒ–å‡çº§çš„æ”¿ç­–è¦æ±‚ï¼Œä¹Ÿæ˜¯æˆ‘ä»¬ç«‹è¶³è¡Œä¸šå®é™…ã€ç¨³æ­¥æ¨è¿›äº§ä¸šç°ä»£åŒ–çš„é‡è¦è·¯å¾„ã€‚å…¬å¸ä»¥æ‰“é€ é«˜ç«¯æ™ºèƒ½å¼€é‡‡æ§åˆ¶æŠ€æœ¯è£…å¤‡äº§å“ä¸ºä¸»ï¼Œç ”å‘ã€åˆ¶é€ ç»¼é‡‡å·¥ä½œé¢æ¶²å‹æ”¯æ¶ç”µæ¶²æ§ç³»ç»Ÿã€æ™ºèƒ½åŒ–æ§åˆ¶ç³»ç»Ÿã€é›†ä¸­ä¾›æ¶²ç³»ç»Ÿã€é«˜ç«¯æ™ºèƒ½ä¹³åŒ–æ¶²æ³µç«™ï¼Œé«˜ç«¯æ™ºèƒ½å–·é›¾æ³µç«™ã€‚
                    """
                ),
                attributes={
                    "æˆç«‹æ—¶é—´": "2021å¹´9æœˆ30æ—¥",
                    "æ³¨å†Œåœ°": "é•¿æ²»å¸‚ç»æµæŠ€æœ¯å¼€å‘åŒº",
                    "æ³¨å†Œèµ„æœ¬": "10000ä¸‡å…ƒ",
                    "è¡Œä¸šé¢†åŸŸ": "ç…¤çŸ¿æœºæ¢°",
                    "å…¬å¸ä½¿å‘½": "æ¨åŠ¨ç…¤æœºè£…å¤‡å‡çº§å’Œåˆ¶é€ æ¨¡å¼åˆ›æ–°",
                    "æŠ€æœ¯åˆ›æ–°æ–¹å‘": "æ™ºèƒ½åŒ–ã€ç»¿è‰²åŒ–è½¬å‹",
                    "äº§å“æ–¹å‘": "æ‰“é€ é«˜ç«¯æ™ºèƒ½å¼€é‡‡æ§åˆ¶æŠ€æœ¯è£…å¤‡äº§å“",
                    "ä¸»è¦äº§å“": [
                    "ç»¼é‡‡å·¥ä½œé¢æ¶²å‹æ”¯æ¶ç”µæ¶²æ§ç³»ç»Ÿ",
                    "æ™ºèƒ½åŒ–æ§åˆ¶ç³»ç»Ÿ",
                    "é›†ä¸­ä¾›æ¶²ç³»ç»Ÿ",
                    "é«˜ç«¯æ™ºèƒ½ä¹³åŒ–æ¶²æ³µç«™",
                    "é«˜ç«¯æ™ºèƒ½å–·é›¾æ³µç«™"
                    ],
                    "å…¬å¸æ„¿æ™¯": "åŠ©åŠ›è¡Œä¸šå¯æŒç»­å‘å±•",
                    "æ”¿ç­–ç¬¦åˆæ€§": "ç¬¦åˆå›½å®¶æ¨åŠ¨åˆ¶é€ ä¸šæ™ºèƒ½åŒ–ã€ç»¿è‰²åŒ–å‡çº§çš„æ”¿ç­–è¦æ±‚",
                    "äº§ä¸šç°ä»£åŒ–è·¯å¾„": "ç«‹è¶³è¡Œä¸šå®é™…ã€ç¨³æ­¥æ¨è¿›äº§ä¸šç°ä»£åŒ–"
                }
            ),
            
        ]
    )
]

def read_docx_file(file_path):
    """è¯»å– DOCX æ–‡ä»¶å†…å®¹"""
    try:
        doc = docx.Document(file_path)
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        return text.strip()
    except Exception as e:
        print(f"âŒ è¯»å– DOCX æ–‡ä»¶å¤±è´¥: {e}")
        return None


def normalize_quotes(text):
    """æ ‡å‡†åŒ–å¼•å·ï¼Œå°†ä¸­æ–‡å¼•å·è½¬æ¢ä¸ºè‹±æ–‡å¼•å·"""
    # ä¸­æ–‡å¼•å·åˆ°è‹±æ–‡å¼•å·çš„æ˜ å°„
    quote_mapping = {
        '"': '"',  # ä¸­æ–‡åŒå¼•å·å·¦ -> è‹±æ–‡åŒå¼•å·
        '"': '"',  # ä¸­æ–‡åŒå¼•å·å³ -> è‹±æ–‡åŒå¼•å·
        ''': "'",  # ä¸­æ–‡å•å¼•å·å·¦ -> è‹±æ–‡å•å¼•å·
        ''': "'",  # ä¸­æ–‡å•å¼•å·å³ -> è‹±æ–‡å•å¼•å·
    }
    
    for chinese_quote, english_quote in quote_mapping.items():
        text = text.replace(chinese_quote, english_quote)
    
    return text


def run_docx_extraction(model_id="qwen3:8b", temperature=0.3, docx_path="/app/test_data/AIæ•°å­—äºº.docx"):
    """ä¸“é—¨å¤„ç† DOCX æ–‡ä»¶çš„æå–å‡½æ•°"""
    print(f"ğŸ“„ æ­£åœ¨å¤„ç† DOCX æ–‡ä»¶: {docx_path}")
    
    # è¯»å– DOCX æ–‡ä»¶
    text_content = read_docx_file(docx_path)
    if text_content is None:
        raise FileNotFoundError(f"æ— æ³•è¯»å–æ–‡ä»¶: {docx_path}")
    
    # æ ‡å‡†åŒ–å¼•å·
    text_content = normalize_quotes(text_content)
    
    print(f"ğŸ“ æ–‡ä»¶å†…å®¹é•¿åº¦: {len(text_content)} å­—ç¬¦")
    print(f"ğŸ“– å†…å®¹é¢„è§ˆ: {text_content[:200]}...")
    
    # é…ç½®æ¨¡å‹
    model_config = lx.factory.ModelConfig(
        model_id=model_id,
        provider_kwargs={
            "model_url": os.getenv("OLLAMA_HOST", "http://localhost:11434"),
            "format_type": lx.data.FormatType.JSON,
            "temperature": temperature,
        },
    )
    
    try:
        # æ‰§è¡Œæå–
        result = lx.extract(
            text_or_documents=text_content,
            prompt_description=prompt,
            examples=examples,
            config=model_config,
            use_schema_constraints=True,
            extraction_passes=2,      # å‡å°‘passesä»¥é¿å…JSONé”™è¯¯ç´¯ç§¯
            max_workers=10,           # å‡å°‘å¹¶å‘ä»¥é¿å…ç«äº‰æ¡ä»¶
            max_char_buffer=500       # å‡å°ç¼“å†²åŒºä»¥æé«˜å‡†ç¡®æ€§
        )
        
        return result
        
    except Exception as e:
        print(f"âš ï¸  æå–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("ğŸ”„ å°è¯•ä½¿ç”¨æ›´ç®€å•çš„é…ç½®é‡æ–°æå–...")
        

def run_literary_extraction(model_id="qwen3:8b", temperature=0.3, text_source="gutenberg"):
    """Run literary text extraction using Ollama."""
    
    if text_source == "gutenberg":
        # Process Romeo & Juliet directly from Project Gutenberg
        print("Downloading and processing Romeo and Juliet from Project Gutenberg...")
        text_or_documents = "https://www.gutenberg.org/files/1513/1513-0.txt"
    elif text_source == "docx":
        # Use local DOCX file
        docx_path = "/app/test_data/AIæ•°å­—äºº.docx"
        print(f"Processing local DOCX file: {docx_path}")
        text_content = read_docx_file(docx_path)
        if text_content is None:
            raise FileNotFoundError(f"æ— æ³•è¯»å–æ–‡ä»¶: {docx_path}")
        text_or_documents = text_content
    else:
        # Use sample text for testing
        text_or_documents = textwrap.dedent("""\
            æ™‹æ§åˆ›åŠ›ï¼ˆå±±è¥¿æ™‹æ§è£…å¤‡åˆ›åŠ›æ™ºèƒ½åˆ¶é€ æœ‰é™å…¬å¸ä»‹ç»ï¼‰æˆç«‹äº2021å¹´9æœˆ30æ—¥ï¼Œæ³¨å†Œåœ°ä½äºé•¿æ²»å¸‚ç»æµæŠ€æœ¯å¼€å‘åŒºï¼Œæ³¨å†Œèµ„æœ¬10000ä¸‡å…ƒã€‚å…¬å¸åœ¨ç…¤çŸ¿æœºæ¢°é¢†åŸŸç§¯ææ¢ç´¢æ™ºèƒ½åŒ–ã€ç»¿è‰²åŒ–è½¬å‹ï¼Œè‡´åŠ›äºæ¨åŠ¨ç…¤æœºè£…å¤‡å‡çº§å’Œåˆ¶é€ æ¨¡å¼åˆ›æ–°ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡æŠ€æœ¯åˆ›æ–°å’Œå·¥è‰ºä¼˜åŒ–ï¼Œæå‡ç…¤æœºäº§å“çš„æ™ºèƒ½åŒ–æ°´å¹³ï¼ŒåŒæ—¶é™ä½èƒ½è€—ä¸æ’æ”¾ï¼ŒåŠ©åŠ›è¡Œä¸šå¯æŒç»­å‘å±•ã€‚è¿™ä¸€æ–¹å‘æ—¢ç¬¦åˆå›½å®¶æ¨åŠ¨åˆ¶é€ ä¸šæ™ºèƒ½åŒ–ã€ç»¿è‰²åŒ–å‡çº§çš„æ”¿ç­–è¦æ±‚ï¼Œä¹Ÿæ˜¯æˆ‘ä»¬ç«‹è¶³è¡Œä¸šå®é™…ã€ç¨³æ­¥æ¨è¿›äº§ä¸šç°ä»£åŒ–çš„é‡è¦è·¯å¾„ã€‚å…¬å¸ä»¥æ‰“é€ é«˜ç«¯æ™ºèƒ½å¼€é‡‡æ§åˆ¶æŠ€æœ¯è£…å¤‡äº§å“ä¸ºä¸»ï¼Œç ”å‘ã€åˆ¶é€ ç»¼é‡‡å·¥ä½œé¢æ¶²å‹æ”¯æ¶ç”µæ¶²æ§ç³»ç»Ÿã€æ™ºèƒ½åŒ–æ§åˆ¶ç³»ç»Ÿã€é›†ä¸­ä¾›æ¶²ç³»ç»Ÿã€é«˜ç«¯æ™ºèƒ½ä¹³åŒ–æ¶²æ³µç«™ï¼Œé«˜ç«¯æ™ºèƒ½å–·é›¾æ³µç«™ã€‚""")
        print("Processing sample Chinese text...")

    model_config = lx.factory.ModelConfig(
        model_id=model_id,
        provider_kwargs={
            "model_url": os.getenv("OLLAMA_HOST", "http://localhost:11434"),
            "format_type": lx.data.FormatType.JSON,
            "temperature": temperature,
        },
    )

    result = lx.extract(
        text_or_documents=text_or_documents,
        prompt_description=prompt,
        examples=examples,
        config=model_config,
        use_schema_constraints=True,
        extraction_passes=3,      # Multiple passes for improved recall
        max_workers=20,           # Parallel processing for speed
        max_char_buffer=1000      # Smaller contexts for better accuracy
    )

    return result


def save_and_visualize_results(result, model_id, temperature, text_source, docx_path=None):
    """Save results to files and generate interactive visualization."""
    
    # ä»æ–‡æ¡£è·¯å¾„ä¸­æå–æ–‡æ¡£åç§°
    if docx_path and os.path.exists(docx_path):
        # ä»å®Œæ•´è·¯å¾„ä¸­æå–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        doc_name = os.path.splitext(os.path.basename(docx_path))[0]
    else:
        # å¦‚æœæ²¡æœ‰æ–‡æ¡£è·¯å¾„ï¼Œä½¿ç”¨text_sourceä½œä¸ºæ–‡æ¡£åç§°
        doc_name = text_source
    
    # æ¸…ç†æ–‡æ¡£åç§°ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦
    doc_name = doc_name.replace(' ', '_').replace('/', '_').replace('\\', '_')
    
    # æ¸…ç†æ¨¡å‹IDï¼Œç§»é™¤å†’å·
    clean_model_id = model_id.replace(':', '_')
    
    # ç”Ÿæˆæ–‡ä»¶åï¼š{æ–‡æ¡£åç§°}_{æ¨¡å‹ID}
    base_filename = f"{doc_name}_{clean_model_id}"
    
    # Save as JSONL format for visualization
    jsonl_file = f"{base_filename}.jsonl"
    lx.io.save_annotated_documents([result], output_name=jsonl_file, output_dir=".")
    print(f"ğŸ“Š JSONL æ–‡ä»¶å·²ä¿å­˜åˆ°: {jsonl_file}")
    
    # Generate the interactive visualization
    try:
        print("ğŸ¨ æ­£åœ¨ç”Ÿæˆäº¤äº’å¼å¯è§†åŒ–...")
        html_content = lx.visualize(jsonl_file)
        
        html_file = f"{base_filename}_visualization.html"
        with open(html_file, "w", encoding='utf-8') as f:
            if hasattr(html_content, 'data'):
                f.write(html_content.data)  # For Jupyter/Colab
            else:
                f.write(html_content)
        
        print(f"ğŸŒ äº¤äº’å¼å¯è§†åŒ–å·²ä¿å­˜åˆ°: {html_file}")
        print(f"   è¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ {html_file} æŸ¥çœ‹å¯è§†åŒ–ç»“æœ")
        
    except Exception as e:
        print(f"âš ï¸  å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
        print("   ä½† JSONL æ–‡ä»¶å·²æˆåŠŸä¿å­˜")


def print_extraction_results(result, model_id, text_source):
    """Print extraction results in a formatted way."""
    print(f"Extracted {len(result.extractions)} entities from {len(result.text):,} characters")

    if result.extractions:
        print(f"\nğŸ“ Found {len(result.extractions)} extraction(s):\n")
        
        # Group extractions by class
        extraction_counts = Counter(extraction.extraction_class for extraction in result.extractions)
        print("ğŸ“Š Extraction Summary:")
        for class_name, count in extraction_counts.most_common():
            print(f"   {class_name}: {count}")
        print()
        
        # Show first few extractions
        for i, extraction in enumerate(result.extractions[:5], 1):
            print(f"Extraction {i}:")
            print(f"  Class: {extraction.extraction_class}")
            print(f"  Text: {extraction.extraction_text[:100]}{'...' if len(extraction.extraction_text) > 100 else ''}")
            print(f"  Attributes: {extraction.attributes}")
            if extraction.char_interval:
                print(f"  Position: {extraction.char_interval.start_pos}-{extraction.char_interval.end_pos}")
            print()
        
        if len(result.extractions) > 5:
            print(f"... and {len(result.extractions) - 5} more extractions")
    else:
        print("\nâš ï¸  No extractions found")

    print("âœ… SUCCESS! Literary extraction completed")
    print(f"   Model: {model_id}")
    print(f"   Text source: {text_source}")
    print("   JSON mode: enabled")
    print("   Schema constraints: enabled")
    print("   Parallel processing: enabled")


# ç¤ºä¾‹è°ƒç”¨å‡½æ•°
def run_example():
    """ç¤ºä¾‹ï¼šè¿è¡Œ DOCX æ–‡ä»¶æå–"""
    try:
        print("ğŸš€ Running DOCX extraction with qwen3:8b...")
        print("ğŸ“„ File: /app/test_data/AIæ•°å­—äºº.docx")
        print("-" * 50)
        
        # ä½¿ç”¨ä¸“é—¨çš„ DOCX å¤„ç†å‡½æ•°
        result = run_docx_extraction(
            model_id="qwen3:8b",
            temperature=0.1,
            docx_path="/app/test_data/AIæ•°å­—äºº.docx"
        )
        
        # æ‰“å°ç»“æœ
        print_extraction_results(result, "qwen3:8b", "docx")
        
        # ä¿å­˜å’Œå¯è§†åŒ–ç»“æœ
        save_and_visualize_results(result, "qwen3:8b", 0.3, "docx", "/app/test_data/AIæ•°å­—äºº.docx")
        
        return True
        
    except ConnectionError as e:
        print(f"\nâŒ ConnectionError: {e}")
        print("\nğŸ’¡ Make sure Ollama is running:")
        print("   ollama serve")
        return False
    except ValueError as e:
        if "Can't find Ollama" in str(e):
            print(f"\nâŒ Model not found: qwen3:8b")
            print("\nğŸ’¡ Install the model first:")
            print("   ollama pull qwen3:8b")
        else:
            print(f"\nâŒ ValueError: {e}")
        return False
    except Exception as e:
        print(f"\nâŒ Error: {type(e).__name__}: {e}")
        return False


if __name__ == "__main__":
    run_example()