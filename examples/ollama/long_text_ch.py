import langextract as lx
import textwrap
import os
from collections import Counter, defaultdict
import docx

# Define comprehensive prompt and examples for complex literary text
prompt = textwrap.dedent("""\
    æå–å‡ºå…¬å¸ã€äº§å“çš„åç§°ã€ç”¨é€”ã€æŠ€æœ¯å‚æ•°ã€ä½¿ç”¨æ–¹æ³•ã€ç‰¹ç‚¹ã€ä¼˜åŠ¿ç­‰å¯¹å…¬å¸å’Œäº§å“è¿›è¡Œä»‹ç»çš„ä¿¡æ¯ã€‚
    """)

examples = [
    lx.data.ExampleData(
        text=textwrap.dedent("""\
            å±±è¥¿äº‘æ™Ÿç§‘æŠ€æœ‰é™å…¬å¸æˆç«‹äº2015å¹´ï¼Œåè½äºæœ‰å¤ªè¡Œæ˜ç ä¹‹ç§°çš„å±±è¥¿çœæ™‹åŸå¸‚ï¼Œå¹¶å…ˆååœ¨æ­¦æ±‰å¸‚ã€è¥¿å®‰å¸‚ã€é•¿æ²»å¸‚ã€ä¸´æ±¾å¸‚ã€é˜³æ³‰å¸‚æˆç«‹å­åˆ†å…¬å¸ã€‚å…¬å¸æ³¨å†Œèµ„é‡‘1000ä¸‡ã€‚
            """),
        extractions=[
            lx.data.Extraction(
                extraction_class="å…¬å¸ä»‹ç»",
                extraction_text=(
                    "å±±è¥¿äº‘æ™Ÿç§‘æŠ€æœ‰é™å…¬å¸æˆç«‹äº2015å¹´ï¼Œåè½äºæœ‰å¤ªè¡Œæ˜ç ä¹‹ç§°çš„å±±è¥¿çœæ™‹åŸå¸‚ï¼Œå¹¶å…ˆååœ¨æ­¦æ±‰å¸‚ã€è¥¿å®‰å¸‚ã€é•¿æ²»å¸‚ã€ä¸´æ±¾å¸‚ã€é˜³æ³‰å¸‚æˆç«‹å­åˆ†å…¬å¸ã€‚å…¬å¸æ³¨å†Œèµ„é‡‘1000ä¸‡ã€‚        äº‘æ™Ÿç§‘æŠ€æ˜¯å›½å†…è§£å†³å·¥ä¸šè¡Œä¸šå®æ“åŸ¹è®­çš„ç§‘æŠ€å…¬å¸ã€‚å…¬å¸è‡´åŠ›äºå°†AI+XR(VRã€MRã€APPç­‰)é«˜å°–ç«¯æŠ€æœ¯åº”ç”¨äºå®‰å…¨æ•™è‚²ä¸åŸ¹è®­é¢†åŸŸï¼Œå¸®åŠ©ä¼ä¸šå’Œä¸ªäººæ›´é«˜æ•ˆã€å®‰å…¨ã€çœŸå®çš„ä½“éªŒã€å­¦ä¹ ã€‚     "
                ),
                attributes={
                    "name": "å±±è¥¿äº‘æ™Ÿç§‘æŠ€æœ‰é™å…¬å¸",
                    "genre": "äº‘æ™Ÿç§‘æŠ€æ˜¯å›½å†…è§£å†³å·¥ä¸šè¡Œä¸šå®æ“åŸ¹è®­çš„ç§‘æŠ€å…¬å¸",
                    "establishment_date": "2015 å¹´",
                    "location": "å±±è¥¿çœæ™‹åŸå¸‚",
                    "registered_capital": "1000 ä¸‡",
                },
            )
        ]
    )
]

def read_docx_file(file_path):
    """è¯»å– DOCX æ–‡ä»¶å†…å®¹"""
    try:
        doc = docx.Document(file_path)
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        return text.strip()
    except Exception as e:
        print(f"âŒ è¯»å– DOCX æ–‡ä»¶å¤±è´¥: {e}")
        return None


def normalize_quotes(text):
    """æ ‡å‡†åŒ–å¼•å·ï¼Œå°†ä¸­æ–‡å¼•å·è½¬æ¢ä¸ºè‹±æ–‡å¼•å·"""
    # ä¸­æ–‡å¼•å·åˆ°è‹±æ–‡å¼•å·çš„æ˜ å°„
    quote_mapping = {
        '"': '"',  # ä¸­æ–‡åŒå¼•å·å·¦ -> è‹±æ–‡åŒå¼•å·
        '"': '"',  # ä¸­æ–‡åŒå¼•å·å³ -> è‹±æ–‡åŒå¼•å·
        ''': "'",  # ä¸­æ–‡å•å¼•å·å·¦ -> è‹±æ–‡å•å¼•å·
        ''': "'",  # ä¸­æ–‡å•å¼•å·å³ -> è‹±æ–‡å•å¼•å·
    }
    
    for chinese_quote, english_quote in quote_mapping.items():
        text = text.replace(chinese_quote, english_quote)
    
    return text


def run_docx_extraction(model_id="qwen3:8b", temperature=0.3, docx_path="/app/test_data/æ™‹æ§åˆ›åŠ›äº§å“ä»‹ç»æ–‡å­—ç‰ˆ.docx"):
    """ä¸“é—¨å¤„ç† DOCX æ–‡ä»¶çš„æå–å‡½æ•°"""
    print(f"ğŸ“„ æ­£åœ¨å¤„ç† DOCX æ–‡ä»¶: {docx_path}")
    
    # è¯»å– DOCX æ–‡ä»¶
    text_content = read_docx_file(docx_path)
    if text_content is None:
        raise FileNotFoundError(f"æ— æ³•è¯»å–æ–‡ä»¶: {docx_path}")
    
    # æ ‡å‡†åŒ–å¼•å·
    text_content = normalize_quotes(text_content)
    
    print(f"ğŸ“ æ–‡ä»¶å†…å®¹é•¿åº¦: {len(text_content)} å­—ç¬¦")
    print(f"ğŸ“– å†…å®¹é¢„è§ˆ: {text_content[:200]}...")
    
    # é…ç½®æ¨¡å‹
    model_config = lx.factory.ModelConfig(
        model_id=model_id,
        provider_kwargs={
            "model_url": os.getenv("OLLAMA_HOST", "http://localhost:11434"),
            "format_type": lx.data.FormatType.JSON,
            "temperature": temperature,
        },
    )
    
    try:
        # æ‰§è¡Œæå–
        result = lx.extract(
            text_or_documents=text_content,
            prompt_description=prompt,
            examples=examples,
            config=model_config,
            use_schema_constraints=True,
            extraction_passes=2,      # å‡å°‘passesä»¥é¿å…JSONé”™è¯¯ç´¯ç§¯
            max_workers=10,           # å‡å°‘å¹¶å‘ä»¥é¿å…ç«äº‰æ¡ä»¶
            max_char_buffer=500       # å‡å°ç¼“å†²åŒºä»¥æé«˜å‡†ç¡®æ€§
        )
        
        return result
        
    except Exception as e:
        print(f"âš ï¸  æå–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("ğŸ”„ å°è¯•ä½¿ç”¨æ›´ç®€å•çš„é…ç½®é‡æ–°æå–...")
        

def run_literary_extraction(model_id="qwen3:8b", temperature=0.3, text_source="gutenberg"):
    """Run literary text extraction using Ollama."""
    
    if text_source == "gutenberg":
        # Process Romeo & Juliet directly from Project Gutenberg
        print("Downloading and processing Romeo and Juliet from Project Gutenberg...")
        text_or_documents = "https://www.gutenberg.org/files/1513/1513-0.txt"
    elif text_source == "docx":
        # Use local DOCX file
        docx_path = "/app/test_data/æ™‹æ§åˆ›åŠ›äº§å“ä»‹ç»æ–‡å­—ç‰ˆ.docx"
        print(f"Processing local DOCX file: {docx_path}")
        text_content = read_docx_file(docx_path)
        if text_content is None:
            raise FileNotFoundError(f"æ— æ³•è¯»å–æ–‡ä»¶: {docx_path}")
        text_or_documents = text_content
    else:
        # Use sample text for testing
        text_or_documents = textwrap.dedent("""\
            æ™‹æ§åˆ›åŠ›ï¼ˆå±±è¥¿æ™‹æ§è£…å¤‡åˆ›åŠ›æ™ºèƒ½åˆ¶é€ æœ‰é™å…¬å¸ä»‹ç»ï¼‰æˆç«‹äº2021å¹´9æœˆ30æ—¥ï¼Œæ³¨å†Œåœ°ä½äºé•¿æ²»å¸‚ç»æµæŠ€æœ¯å¼€å‘åŒºï¼Œæ³¨å†Œèµ„æœ¬10000ä¸‡å…ƒã€‚å…¬å¸åœ¨ç…¤çŸ¿æœºæ¢°é¢†åŸŸç§¯ææ¢ç´¢æ™ºèƒ½åŒ–ã€ç»¿è‰²åŒ–è½¬å‹ï¼Œè‡´åŠ›äºæ¨åŠ¨ç…¤æœºè£…å¤‡å‡çº§å’Œåˆ¶é€ æ¨¡å¼åˆ›æ–°ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡æŠ€æœ¯åˆ›æ–°å’Œå·¥è‰ºä¼˜åŒ–ï¼Œæå‡ç…¤æœºäº§å“çš„æ™ºèƒ½åŒ–æ°´å¹³ï¼ŒåŒæ—¶é™ä½èƒ½è€—ä¸æ’æ”¾ï¼ŒåŠ©åŠ›è¡Œä¸šå¯æŒç»­å‘å±•ã€‚è¿™ä¸€æ–¹å‘æ—¢ç¬¦åˆå›½å®¶æ¨åŠ¨åˆ¶é€ ä¸šæ™ºèƒ½åŒ–ã€ç»¿è‰²åŒ–å‡çº§çš„æ”¿ç­–è¦æ±‚ï¼Œä¹Ÿæ˜¯æˆ‘ä»¬ç«‹è¶³è¡Œä¸šå®é™…ã€ç¨³æ­¥æ¨è¿›äº§ä¸šç°ä»£åŒ–çš„é‡è¦è·¯å¾„ã€‚å…¬å¸ä»¥æ‰“é€ é«˜ç«¯æ™ºèƒ½å¼€é‡‡æ§åˆ¶æŠ€æœ¯è£…å¤‡äº§å“ä¸ºä¸»ï¼Œç ”å‘ã€åˆ¶é€ ç»¼é‡‡å·¥ä½œé¢æ¶²å‹æ”¯æ¶ç”µæ¶²æ§ç³»ç»Ÿã€æ™ºèƒ½åŒ–æ§åˆ¶ç³»ç»Ÿã€é›†ä¸­ä¾›æ¶²ç³»ç»Ÿã€é«˜ç«¯æ™ºèƒ½ä¹³åŒ–æ¶²æ³µç«™ï¼Œé«˜ç«¯æ™ºèƒ½å–·é›¾æ³µç«™ã€‚""")
        print("Processing sample Chinese text...")

    model_config = lx.factory.ModelConfig(
        model_id=model_id,
        provider_kwargs={
            "model_url": os.getenv("OLLAMA_HOST", "http://localhost:11434"),
            "format_type": lx.data.FormatType.JSON,
            "temperature": temperature,
        },
    )

    result = lx.extract(
        text_or_documents=text_or_documents,
        prompt_description=prompt,
        examples=examples,
        config=model_config,
        use_schema_constraints=True,
        extraction_passes=3,      # Multiple passes for improved recall
        max_workers=20,           # Parallel processing for speed
        max_char_buffer=1000      # Smaller contexts for better accuracy
    )

    return result


def save_and_visualize_results(result, model_id, temperature, text_source):
    """Save results to files and generate interactive visualization."""
    
    # Save as JSONL format for visualization
    jsonl_file = f"literary_extraction_{model_id.replace(':', '_')}.jsonl"
    lx.io.save_annotated_documents([result], output_name=jsonl_file, output_dir=".")
    print(f"ğŸ“Š JSONL æ–‡ä»¶å·²ä¿å­˜åˆ°: {jsonl_file}")
    
    # Generate the interactive visualization
    try:
        print("ğŸ¨ æ­£åœ¨ç”Ÿæˆäº¤äº’å¼å¯è§†åŒ–...")
        html_content = lx.visualize(jsonl_file)
        
        html_file = f"literary_visualization_{model_id.replace(':', '_')}.html"
        with open(html_file, "w", encoding='utf-8') as f:
            if hasattr(html_content, 'data'):
                f.write(html_content.data)  # For Jupyter/Colab
            else:
                f.write(html_content)
        
        print(f"ğŸŒ äº¤äº’å¼å¯è§†åŒ–å·²ä¿å­˜åˆ°: {html_file}")
        print(f"   è¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ {html_file} æŸ¥çœ‹å¯è§†åŒ–ç»“æœ")
        
    except Exception as e:
        print(f"âš ï¸  å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
        print("   ä½† JSONL æ–‡ä»¶å·²æˆåŠŸä¿å­˜")


def print_extraction_results(result, model_id, text_source):
    """Print extraction results in a formatted way."""
    print(f"Extracted {len(result.extractions)} entities from {len(result.text):,} characters")

    if result.extractions:
        print(f"\nğŸ“ Found {len(result.extractions)} extraction(s):\n")
        
        # Group extractions by class
        extraction_counts = Counter(extraction.extraction_class for extraction in result.extractions)
        print("ğŸ“Š Extraction Summary:")
        for class_name, count in extraction_counts.most_common():
            print(f"   {class_name}: {count}")
        print()
        
        # Show first few extractions
        for i, extraction in enumerate(result.extractions[:5], 1):
            print(f"Extraction {i}:")
            print(f"  Class: {extraction.extraction_class}")
            print(f"  Text: {extraction.extraction_text[:100]}{'...' if len(extraction.extraction_text) > 100 else ''}")
            print(f"  Attributes: {extraction.attributes}")
            if extraction.char_interval:
                print(f"  Position: {extraction.char_interval.start_pos}-{extraction.char_interval.end_pos}")
            print()
        
        if len(result.extractions) > 5:
            print(f"... and {len(result.extractions) - 5} more extractions")
    else:
        print("\nâš ï¸  No extractions found")

    print("âœ… SUCCESS! Literary extraction completed")
    print(f"   Model: {model_id}")
    print(f"   Text source: {text_source}")
    print("   JSON mode: enabled")
    print("   Schema constraints: enabled")
    print("   Parallel processing: enabled")


# ç¤ºä¾‹è°ƒç”¨å‡½æ•°
def run_example():
    """ç¤ºä¾‹ï¼šè¿è¡Œ DOCX æ–‡ä»¶æå–"""
    try:
        print("ğŸš€ Running DOCX extraction with qwen3:8b...")
        print("ğŸ“„ File: /app/test_data/æ™‹æ§åˆ›åŠ›äº§å“ä»‹ç»æ–‡å­—ç‰ˆ.docx")
        print("-" * 50)
        
        # ä½¿ç”¨ä¸“é—¨çš„ DOCX å¤„ç†å‡½æ•°
        result = run_docx_extraction(
            model_id="qwen3:8b",
            temperature=0.1,
            docx_path="/app/test_data/æ™‹æ§åˆ›åŠ›äº§å“ä»‹ç»æ–‡å­—ç‰ˆ.docx"
        )
        
        # æ‰“å°ç»“æœ
        print_extraction_results(result, "qwen3:8b", "docx")
        
        # ä¿å­˜å’Œå¯è§†åŒ–ç»“æœ
        save_and_visualize_results(result, "qwen3:8b", 0.3, "docx")
        
        return True
        
    except ConnectionError as e:
        print(f"\nâŒ ConnectionError: {e}")
        print("\nğŸ’¡ Make sure Ollama is running:")
        print("   ollama serve")
        return False
    except ValueError as e:
        if "Can't find Ollama" in str(e):
            print(f"\nâŒ Model not found: qwen3:8b")
            print("\nğŸ’¡ Install the model first:")
            print("   ollama pull qwen3:8b")
        else:
            print(f"\nâŒ ValueError: {e}")
        return False
    except Exception as e:
        print(f"\nâŒ Error: {type(e).__name__}: {e}")
        return False


if __name__ == "__main__":
    run_example()